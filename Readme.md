# üß† Advance Rag: Reinforcement Learning with Active Neural Generative Coding

Welcome to the **Advance Rag** demonstration! This project showcases an innovative approach to reinforcement learning (RL) using **Active Neural Generative Coding (ANGC)**. Our goal is to develop intelligent agents capable of learning and adapting to dynamic environments without relying on traditional backpropagation methods.

## üöÄ Overview

The **Advance Rag** framework integrates state-of-the-art techniques in RL and generative modeling, allowing agents to make decisions based on both exploration and exploitation. This approach is inspired by cognitive theories of learning and aims to mimic human-like decision-making processes.

### Key Features

- **Backprop-Free Learning**: Leverage ANGC to train agents without the need for backpropagation, enhancing efficiency and biological relevance.
- **Dynamic Environment Interaction**: Agents learn to adapt to their surroundings through active inference, balancing exploration and goal-directed behavior.
- **Robust Performance**: Demonstrated effectiveness across various control problems, including the inverted pendulum, mountain car, lunar lander, and robotic control tasks.

## üìö Getting Started

### Prerequisites

To run the **Advance Rag** demonstration, ensure you have the following installed:

- Python 3.11 or higher
- Required libraries (listed in `requirements.txt`)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/Advance-Rag.git
   cd Advance-Rag
   ```

2. Create a virtual environment and activate it:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows use .venv\Scripts\activate
   ```

3. Install the required packages

### Running the Demonstration

To run the demonstration, execute the following command in your terminal:

```bash
jupyter notebook Advance\ Rag.ipynb
```

Open the notebook in your browser and follow the instructions to see the ANGC in action!

## üìä Results

The **Advance Rag** framework has been tested on several RL benchmarks, showcasing competitive performance against traditional methods. The results indicate that our approach can effectively tackle control problems with sparse rewards, demonstrating the potential of biologically-inspired learning mechanisms.

### Example Outputs

- **Inverted Pendulum**: Achieved stable balancing with minimal episodes.
- **Mountain Car**: Successfully navigated the environment, overcoming challenges with sparse rewards.
- **Lunar Lander**: Demonstrated effective landing strategies through learned policies.

## ü§ù Contributing

We welcome contributions to enhance the **Advance Rag** project! If you have ideas, suggestions, or improvements, please fork the repository and submit a pull request.

---

Thank you for exploring the **Advance Rag** demonstration! We hope you find it insightful and inspiring for your own projects in reinforcement learning and generative modeling.
