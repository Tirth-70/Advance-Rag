{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the apis are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Model API Check--\n",
      "I am well, thank you for asking. How are you doing today?\n",
      "\n",
      "--Embeddings API Check--\n",
      "[-0.09338379, 0.0871582, -0.03326416, 0.01953125, 0.07702637, 0.034729004, -0.058380127, -0.031021118, -0.030517578, -0.055999756, 0.050842285, -0.006752014, 0.038391113, -0.0014362335, -0.041137695, -0.008880615, 0.026000977, -0.023010254, 0.05456543, -0.03366089, 0.055633545, 0.028579712, -0.068603516, 0.03970337, -0.06677246, 0.06732178, -0.013053894, -0.0060920715, 0.038116455, 0.057800293, 0.048736572, 0.026855469, 0.009849548, 0.08312988, 0.073791504, 0.01663208, -0.0871582, 0.01802063, -0.0020828247, -0.0031356812, 0.039978027, -0.03164673, 0.009796143, 0.011375427, 0.0068855286, 0.092285156, 0.05218506, -0.060943604, 0.038269043, -0.018218994, -0.04510498, -0.0847168, 0.008300781, -0.060058594, 0.0012111664, 0.05102539, 0.05218506, -0.047210693, -0.051239014, -0.044158936, -0.058166504, 0.07849121, -0.019165039, 0.06451416, 0.024887085, 0.011405945, -0.03768921, -0.018814087, -0.06829834, -0.052825928, -0.019104004, -0.021194458, 0.043518066, 0.07525635, 0.082336426, 0.0037651062, -0.0060310364, -0.03265381, 0.011375427, -0.013847351, -0.07232666, 0.02986145, 0.03866577, -0.029083252, 0.008666992, 0.03845215, 0.045196533, 0.012756348, -0.018051147, 0.032440186, -0.030715942, -0.045440674, -0.11187744, 0.032073975, 0.021972656, -0.044921875, -0.030410767, -0.03668213, 0.12420654, 0.05029297, -0.032989502, -0.049438477, 0.001704216, -0.08074951, 0.00046396255, -0.04107666, 0.020599365, -0.089416504, 0.020477295, -0.038726807, -0.04437256, -0.019256592, 0.048583984, 0.046020508, 0.03741455, -0.037475586, -0.050720215, 0.052856445, -0.10229492, -0.00010281801, 0.058776855, 0.021453857, -0.031051636, 0.01676941, 0.024047852, -0.026306152, 0.15258789, -0.09979248, 0.04888916, 0.045166016, 0.008865356, -0.043914795, -0.032928467, 0.0052757263, 0.06072998, 0.036956787, -0.058013916, 0.053466797, -0.03225708, 0.018371582, -0.0042533875, 0.047943115, 0.06530762, 0.039855957, -0.025360107, 0.047332764, -0.15124512, 0.08325195, 0.016174316, -0.029724121, 0.111816406, -0.05230713, -0.06964111, 0.03060913, -0.04257202, -0.0284729, 0.007843018, -0.03866577, 0.07867432, -0.04446411, 0.028869629, -0.015823364, 0.02659607, 0.085754395, 0.03878784, -0.04232788, 0.017074585, 0.026779175, -0.04284668, -0.017105103, 0.10058594, 0.022323608, -0.007007599, -0.09661865, -0.01322937, -0.004627228, 0.057800293, 0.057159424, -0.033294678, -0.066101074, 0.010910034, 0.033569336, -0.062042236, -0.0072021484, -0.070373535, 0.034729004, -0.07434082, -0.06604004, 0.061401367, 0.09576416, -0.070739746, 0.066833496, -0.019042969, -0.0051994324, -0.07696533, -0.03564453, 0.048614502, -0.048919678, 0.036224365, -0.06652832, 0.03338623, 0.05847168, 0.009414673, -0.035095215, 0.011787415, -0.007675171, -0.057006836, -0.045074463, -0.027999878, -0.049102783, -0.025787354, -0.010101318, -0.000813961, -0.009963989, -0.013343811, 0.04046631, 0.02758789, -0.07086182, 0.09442139, -0.012275696, -0.018936157, -0.011940002, 0.10638428, -0.10913086, 0.05606079, 0.008895874, 0.017089844, 0.019958496, 0.03173828, -0.037322998, 0.019699097, 0.046722412, -0.08959961, 0.059448242, 0.018875122, -0.057495117, -0.039276123, 0.009063721, -0.0178833, 0.032073975, -0.08178711, -0.061431885, 0.05731201, 0.012886047, -0.025360107, 0.04498291, 0.027923584, 0.125, 0.013374329, -0.013069153, -0.031677246, -0.109558105, 0.05731201, -0.03765869, 0.04650879, -0.005706787, 0.021697998, -0.0008239746, 0.030090332, -0.048736572, 0.07940674, -0.017120361, 0.018737793, 0.12011719, -0.03564453, 0.07519531, -0.039611816, -0.014968872, -0.045288086, 0.07702637, 0.010681152, -0.04736328, 0.07623291, 0.008071899, 0.080078125, -0.060516357, 0.043426514, -0.026489258, -0.018188477, 0.049560547, -0.068847656, -0.03387451, -0.09661865, -0.03768921, 0.028549194, 0.036621094, 0.05307007, -0.053894043, 0.0019035339, -0.07788086, -0.010597229, -0.027420044, 0.10900879, 0.019302368, -0.06726074, 0.04937744, 0.05154419, -0.050598145, 0.07562256, -0.05569458, 0.073913574, -0.052337646, -0.0149383545, -0.00037050247, 0.037322998, 0.018478394, -0.03201294, -0.04788208, 0.03062439, -0.055786133, 0.0018081665, 0.029510498, -0.10864258, -0.027374268, 0.040405273, 0.01474762, -0.010726929, -0.086242676, -0.02658081, -0.057159424, -0.0095825195, -0.11804199, -0.014289856, -0.006881714, -0.028533936, 0.005382538, -0.053771973, -0.015853882, 0.0034332275, -0.08441162, -0.028182983, -0.00856781, -0.060394287, -0.036590576, 0.03062439, 0.112854004, -0.008041382, -0.03353882, 0.0181427, -0.03466797, 0.026565552, -0.033813477, 0.0074310303, -0.02017212, -0.047729492, 0.00010108948, -0.032073975, 0.08630371, 0.08557129, -0.0115737915, 0.044067383, 0.062042236, 0.00819397, -0.016082764, 0.01574707, 0.0154418945, 0.06726074, 0.056884766, 0.01210022, 0.048095703, -0.0017309189, 0.018295288, -0.00592041, 0.062286377, 0.040649414, -0.032928467, -0.05392456, -0.13891602, -0.033050537, 0.047973633, -0.07824707, 0.024627686, -0.02923584, 0.09118652, 0.0690918, 0.045837402, -0.06402588, -0.028747559, -0.06542969, -0.08496094, 0.06762695, 0.04220581, 0.059539795, 0.0023174286]\n",
      "\n",
      "--Web Search API Check--\n",
      "[{'url': 'https://www.python.org/', 'content': 'jobs.python.org\\nLatest News\\nMore\\nUpcoming Events\\nMore\\nSuccess Stories\\nMore\\nAbridging clinical conversations using Python by Nimshi Venkat and Sandeep Konam\\nUse Python for…\\nMore\\n>>> Python Software Foundation\\nThe mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. More control flow tools in Python\\xa03\\nQuick & Easy to Learn\\nExperienced programmers in any other language can pick up Python very quickly, and beginners find the clean syntax and indentation structure easy to learn. More about lists in Python\\xa03\\nIntuitive Interpretation\\nCalculations are simple with Python, and expression syntax is straightforward: the operators +, -, * and / work as expected; parentheses () can be used for grouping. More about defining functions in Python\\xa03\\nCompound Data Types\\nLists (known as arrays in other languages) are one of the compound data types that Python understands.'}, {'url': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'content': 'Other implementations\\nUnsupported implementations\\nOther just-in-time Python compilers have been developed, but are now unsupported:\\nCross-compilers to other languages\\nThere are several compilers/transpilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\\nSpecialized:\\nOlder projects (or not to be used with Python 3.x and latest syntax):\\nPerformance\\nPerformance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy \\'13.[160] Python\\'s performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game.[161]\\nDevelopment\\nPython\\'s development is conducted largely through the Python Enhancement Proposal (PEP) process, the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions.[162] Python coding style is covered in PEP\\xa08.[163] Outstanding PEPs are reviewed and commented on by the Python community and the steering council.[162]\\nEnhancement of the language corresponds with the development of the CPython reference implementation. Python\\xa02.7.18, released in 2020, was the last release of Python\\xa02.[35]\\nPython consistently ranks as one of the most popular programming languages.[36][37][38][39]\\nHistory\\nPython was conceived in the late 1980s[40] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL,[41] capable of exception handling and interfacing with the Amoeba operating system.[10] Its implementation began in December\\xa01989.[42] Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his \"permanent vacation\" from his responsibilities as Python\\'s \"benevolent dictator for life\", a title the Python community bestowed upon him to reflect his long-term commitment as the project\\'s chief decision-maker.[43] In January\\xa02019, active Python core developers elected a five-member Steering Council to lead the project.[44][45]\\nPython 2.0 was released on 16 October 2000, with many major new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support.[46] Python\\xa03.0, released on 3 December 2008, with many of its major features backported to Python\\xa02.6.x[47] and 2.7.x. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes.[116] The Fraction class in the fractions module provides arbitrary precision for rational numbers.[117]\\nDue to Python\\'s extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.[118][119]\\nProgramming examples\\nHello world program:\\nProgram to calculate the factorial of a positive integer:\\nLibraries\\nPython\\'s large standard library[120] provides tools suited to many tasks and is commonly cited as one of its greatest strengths. SageMath is a computer algebra system with a notebook interface programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus.[197] OpenCV has Python bindings with a rich set of features for computer vision and image processing.[198]\\nPython is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, and scikit-learn.[199][200][201][202] As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing.[203]\\nPython can also be used for graphical user interface (GUI) by using libraries like Tkinter.[204][205]\\nPython can also be used to create games, with libraries such as Pygame, which can make 2D games.\\n This is reflected in its name—a tribute to the British comedy group Monty Python[76]—and in occasionally playful approaches to tutorials and reference materials, such as the use of the terms \"spam\" and \"eggs\" (a reference to a Monty Python sketch) in examples, instead of the often-used \"foo\" and \"bar\".[77][78]\\nA common neologism in the Python community is pythonic, which has a wide range of meanings related to program style.'}, {'url': 'https://docs.python.org/3/tutorial/index.html', 'content': 'After reading it, you will be able to read and\\nwrite Python modules and programs, and you will be ready to learn more about the\\nvarious Python library modules described in The Python Standard Library.\\n The Python interpreter and the extensive standard library are freely available\\nin source or binary form for all major platforms from the Python web site,\\nhttps://www.python.org/, and may be freely distributed. The same site also\\ncontains distributions of and pointers to many free third party Python modules,\\nprograms and tools, and additional documentation.\\n Whetting Your Appetite\\nThis Page\\nNavigation\\nThe Python Tutorial¶\\nPython is an easy to learn, powerful programming language. Python’s elegant syntax and dynamic typing,\\ntogether with its interpreted nature, make it an ideal language for scripting\\nand rapid application development in many areas on most platforms.\\n'}, {'url': 'https://www.python.org/shell/', 'content': \"jobs.python.org\\nLatest News\\nMore\\nUpcoming Events\\nMore\\nSuccess Stories\\nMore\\nPython provides convenience and flexibility for scalable ML/AI by Dean Wampler\\nUse Python for…\\nMore\\n>>> Python Software Foundation\\nThe mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. Learn More\\nGet Started\\nWhether you're new to programming or an experienced developer, it's easy to learn and use Python.\\n Learn more\\nBecome a Member\\nDonate to the PSF\\nCopyright ©2001-2024.\\n Latest: Python 3.12.1\\nDocs\\nDocumentation for Python's standard library, along with tutorials and guides, are available online.\\n\"}, {'url': 'https://www.w3schools.com/python/', 'content': 'Tutorials\\nHTML and CSS\\nData Analytics\\nWeb Building\\nJavaScript\\nWeb Building\\nBackend\\nData Analytics\\nWeb Building\\nExercises\\nHTML and CSS\\nData Analytics\\nJavaScript\\nBackend\\nData Analytics\\nCertificates\\nHTML and CSS\\nData Analytics\\nPrograms\\nJavaScript\\nPrograms\\nPrograms\\nBackend\\nData Analytics\\nAll Our Services\\nW3Schools offers a wide range of services and products for beginners and professionals,\\nhelping millions of people everyday to learn and master new skills.\\n Python Reference\\nYou will also find complete function and method references:\\nReference Overview\\nBuilt-in Functions\\nString Methods\\nList/Array Methods\\nDictionary Methods\\nTuple Methods\\nSet Methods\\nFile Methods\\nPython Keywords\\nPython Exceptions\\nPython Glossary\\nRandom Module\\nRequests Module\\nMath Module\\nCMath Module\\nDownload Python\\nDownload Python from the official Python web site:\\nhttps://python.org\\nKickstart your career\\nGet certified by completing the\\ncourse\\nReport Error\\nIf you want to report an error, or if you want to make a suggestion, do not hesitate to send us an e-mail:\\nhelp@w3schools.com Enjoy our free tutorials like millions of other internet users since 1999\\nExplore our selection of references covering all popular coding languages\\nCreate your own website with\\nW3Schools Spaces\\n- no setup required\\nTest your skills with different exercises\\nTest yourself with multiple choice questions\\nDocument your knowledge\\nCreate a\\nfree\\nW3Schools Account to Improve Your Learning Experience\\nTrack your learning progress at W3Schools and collect rewards\\nBecome a PRO user and unlock powerful features (ad-free, hosting, videos,..)\\n Help the lynx collect pine cones\\nFind Jobs or Hire Talent with W3Schools Pathfinder\\nJoin our newsletter and get access to exclusive content every month\\nPython Tutorial\\nFile Handling\\nPython Modules\\nPython Matplotlib\\nMachine Learning\\nPython MySQL\\nPython MongoDB\\nPython Reference\\nModule Reference\\nPython Large collection of code snippets for HTML, CSS and JavaScript\\nBuild fast and responsive sites using our free\\nW3.CSS\\nframework\\nRead long term trends of browser usage\\nTest your typing speed\\nLearn Amazon Web Services\\nUse our color picker to find different RGB, HEX and HSL colors.\\n'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Set model\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\").invoke(\"Hello, how are you?\")\n",
    "print(\"--Model API Check--\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Set embeddings\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(\"--Embeddings API Check--\")\n",
    "print(query_result)\n",
    "print()\n",
    "\n",
    "# Set web search\n",
    "web_search_tool = TavilySearchResults()\n",
    "search_results = web_search_tool.invoke(\"python\")\n",
    "print(\"--Web Search API Check--\")\n",
    "print(search_results)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up vector store of the given pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a PDF document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf = \"ex.pdf\"\n",
    "pdf_description = \"This is a book about the PHARMACOLOGY-II, B.Pharm, Semester-V According to the syllabus based on 'Pharmacy Council of India'.\"\n",
    "\n",
    "loader = PyPDFLoader(pdf, extract_images=True)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the document into chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore  \n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "index_name = \"pdf-chatter\"\n",
    "embeddings = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a router to pick between tools. \n",
    " \n",
    "Cohere model decides which tool(s) to call, as well as the how to query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '59e50f28-2d46-4239-8916-4750ca84eb27', 'function': {'name': 'web_search', 'arguments': '{\"query\": \"capital of france\"}'}, 'type': 'function'}]\n",
      "[{'id': 'c5798d1b-2435-4db2-b0f1-c93b7460f6cf', 'function': {'name': 'vectorstore', 'arguments': '{\"query\": \"Tirth Patel contact number\"}'}, 'type': 'function'}]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Data model\n",
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "# Preamble\n",
    "preamble = f'''You are a master at finding information! You have two knowledge sources:\n",
    "        1. A massive document storage system containing detailed summaries ({pdf_description}) (vectorstore).\n",
    "        2. web_search.\n",
    "    When a user asks a question, if it directly relates to a document description in the storage system, use vectorstore to answer their question. \n",
    "    Otherwise, web_search for the best answer.'''\n",
    "# LLM with tool use and preamble\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_router = llm.bind_tools(tools=[web_search, vectorstore], preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "response = question_router.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response.response_metadata['tool_calls'])\n",
    "response = question_router.invoke({\"question\": \"What is the contact number of Tirth Patel?\"})\n",
    "print(response.response_metadata['tool_calls'])\n",
    "response = question_router.invoke({\"question\": \"Hi how are you?\"})\n",
    "print('tool_calls' in response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# Prompt\n",
    "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, preamble=preamble)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"What is Deep Learning?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "response =  retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that uses multi-layered artificial neural networks to learn complex patterns from large datasets.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \",\n",
    "            additional_kwargs={\"documents\": x[\"documents\"]},\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have feelings as an AI chatbot, but I'm here to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "### LLM fallback\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"Hi how are you?\"\n",
    "generation = llm_chain.invoke({\"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "\n",
    "Capture the flow in as a graph.\n",
    "\n",
    "## Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"|\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---LLM Fallback---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    if not isinstance(documents, list):\n",
    "      documents = [documents]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    print(generation)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    # Fallback to LLM or raise error if no decision\n",
    "    if \"tool_calls\" not in source.additional_kwargs:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"llm_fallback\"\n",
    "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "      raise \"Router could not decide source\"\n",
    "\n",
    "    # Choose datasource\n",
    "    datasource = source.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "    if datasource == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    else: \n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    print(\"dlsknflsdn\",type(score),score)\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY USING WEBSEARCH---\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # rag\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback) # llm\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate\", # Hallucinations: re-generate \n",
    "        \"not useful\": \"web_search\", # Fails to answer question: fall-back to web-search \n",
    "        \"useful\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "The Chicago Bears are expected to draft quarterback Caleb Williams first in the 2024 NFL draft.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The Chicago Bears are expected to draft quarterback Caleb Williams first in '\n",
      " 'the 2024 NFL draft.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What player are the Bears expected to draft first in the 2024 NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/623da7bb-84a7-4e53-a63e-7ccd77fb9be5/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "Insulin affects the liver, skeletal muscle, and lipid tissue. Its actions are summarised on page 206.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Insulin affects the liver, skeletal muscle, and lipid tissue. Its actions '\n",
      " 'are summarised on page 206.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What is Physiological Actions of Insulin? according to the organs and there effects. also mention on which page this information is stored.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/57f3973b-6879-4fbe-ae31-9ae524c3a697/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "Paris is the capital of France.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'Paris is the capital of France.'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What is the capital of France?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
