{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the apis are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Model API Check--\n",
      "I am well, thank you for asking. How are you doing today?\n",
      "\n",
      "--Embeddings API Check--\n",
      "[-0.09338379, 0.0871582, -0.03326416, 0.01953125, 0.07702637, 0.034729004, -0.058380127, -0.031021118, -0.030517578, -0.055999756, 0.050842285, -0.006752014, 0.038391113, -0.0014362335, -0.041137695, -0.008880615, 0.026000977, -0.023010254, 0.05456543, -0.03366089, 0.055633545, 0.028579712, -0.068603516, 0.03970337, -0.06677246, 0.06732178, -0.013053894, -0.0060920715, 0.038116455, 0.057800293, 0.048736572, 0.026855469, 0.009849548, 0.08312988, 0.073791504, 0.01663208, -0.0871582, 0.01802063, -0.0020828247, -0.0031356812, 0.039978027, -0.03164673, 0.009796143, 0.011375427, 0.0068855286, 0.092285156, 0.05218506, -0.060943604, 0.038269043, -0.018218994, -0.04510498, -0.0847168, 0.008300781, -0.060058594, 0.0012111664, 0.05102539, 0.05218506, -0.047210693, -0.051239014, -0.044158936, -0.058166504, 0.07849121, -0.019165039, 0.06451416, 0.024887085, 0.011405945, -0.03768921, -0.018814087, -0.06829834, -0.052825928, -0.019104004, -0.021194458, 0.043518066, 0.07525635, 0.082336426, 0.0037651062, -0.0060310364, -0.03265381, 0.011375427, -0.013847351, -0.07232666, 0.02986145, 0.03866577, -0.029083252, 0.008666992, 0.03845215, 0.045196533, 0.012756348, -0.018051147, 0.032440186, -0.030715942, -0.045440674, -0.11187744, 0.032073975, 0.021972656, -0.044921875, -0.030410767, -0.03668213, 0.12420654, 0.05029297, -0.032989502, -0.049438477, 0.001704216, -0.08074951, 0.00046396255, -0.04107666, 0.020599365, -0.089416504, 0.020477295, -0.038726807, -0.04437256, -0.019256592, 0.048583984, 0.046020508, 0.03741455, -0.037475586, -0.050720215, 0.052856445, -0.10229492, -0.00010281801, 0.058776855, 0.021453857, -0.031051636, 0.01676941, 0.024047852, -0.026306152, 0.15258789, -0.09979248, 0.04888916, 0.045166016, 0.008865356, -0.043914795, -0.032928467, 0.0052757263, 0.06072998, 0.036956787, -0.058013916, 0.053466797, -0.03225708, 0.018371582, -0.0042533875, 0.047943115, 0.06530762, 0.039855957, -0.025360107, 0.047332764, -0.15124512, 0.08325195, 0.016174316, -0.029724121, 0.111816406, -0.05230713, -0.06964111, 0.03060913, -0.04257202, -0.0284729, 0.007843018, -0.03866577, 0.07867432, -0.04446411, 0.028869629, -0.015823364, 0.02659607, 0.085754395, 0.03878784, -0.04232788, 0.017074585, 0.026779175, -0.04284668, -0.017105103, 0.10058594, 0.022323608, -0.007007599, -0.09661865, -0.01322937, -0.004627228, 0.057800293, 0.057159424, -0.033294678, -0.066101074, 0.010910034, 0.033569336, -0.062042236, -0.0072021484, -0.070373535, 0.034729004, -0.07434082, -0.06604004, 0.061401367, 0.09576416, -0.070739746, 0.066833496, -0.019042969, -0.0051994324, -0.07696533, -0.03564453, 0.048614502, -0.048919678, 0.036224365, -0.06652832, 0.03338623, 0.05847168, 0.009414673, -0.035095215, 0.011787415, -0.007675171, -0.057006836, -0.045074463, -0.027999878, -0.049102783, -0.025787354, -0.010101318, -0.000813961, -0.009963989, -0.013343811, 0.04046631, 0.02758789, -0.07086182, 0.09442139, -0.012275696, -0.018936157, -0.011940002, 0.10638428, -0.10913086, 0.05606079, 0.008895874, 0.017089844, 0.019958496, 0.03173828, -0.037322998, 0.019699097, 0.046722412, -0.08959961, 0.059448242, 0.018875122, -0.057495117, -0.039276123, 0.009063721, -0.0178833, 0.032073975, -0.08178711, -0.061431885, 0.05731201, 0.012886047, -0.025360107, 0.04498291, 0.027923584, 0.125, 0.013374329, -0.013069153, -0.031677246, -0.109558105, 0.05731201, -0.03765869, 0.04650879, -0.005706787, 0.021697998, -0.0008239746, 0.030090332, -0.048736572, 0.07940674, -0.017120361, 0.018737793, 0.12011719, -0.03564453, 0.07519531, -0.039611816, -0.014968872, -0.045288086, 0.07702637, 0.010681152, -0.04736328, 0.07623291, 0.008071899, 0.080078125, -0.060516357, 0.043426514, -0.026489258, -0.018188477, 0.049560547, -0.068847656, -0.03387451, -0.09661865, -0.03768921, 0.028549194, 0.036621094, 0.05307007, -0.053894043, 0.0019035339, -0.07788086, -0.010597229, -0.027420044, 0.10900879, 0.019302368, -0.06726074, 0.04937744, 0.05154419, -0.050598145, 0.07562256, -0.05569458, 0.073913574, -0.052337646, -0.0149383545, -0.00037050247, 0.037322998, 0.018478394, -0.03201294, -0.04788208, 0.03062439, -0.055786133, 0.0018081665, 0.029510498, -0.10864258, -0.027374268, 0.040405273, 0.01474762, -0.010726929, -0.086242676, -0.02658081, -0.057159424, -0.0095825195, -0.11804199, -0.014289856, -0.006881714, -0.028533936, 0.005382538, -0.053771973, -0.015853882, 0.0034332275, -0.08441162, -0.028182983, -0.00856781, -0.060394287, -0.036590576, 0.03062439, 0.112854004, -0.008041382, -0.03353882, 0.0181427, -0.03466797, 0.026565552, -0.033813477, 0.0074310303, -0.02017212, -0.047729492, 0.00010108948, -0.032073975, 0.08630371, 0.08557129, -0.0115737915, 0.044067383, 0.062042236, 0.00819397, -0.016082764, 0.01574707, 0.0154418945, 0.06726074, 0.056884766, 0.01210022, 0.048095703, -0.0017309189, 0.018295288, -0.00592041, 0.062286377, 0.040649414, -0.032928467, -0.05392456, -0.13891602, -0.033050537, 0.047973633, -0.07824707, 0.024627686, -0.02923584, 0.09118652, 0.0690918, 0.045837402, -0.06402588, -0.028747559, -0.06542969, -0.08496094, 0.06762695, 0.04220581, 0.059539795, 0.0023174286]\n",
      "\n",
      "--Web Search API Check--\n",
      "[{'url': 'https://docs.python.org/3/tutorial/index.html', 'content': 'After reading it, you will be able to read and\\nwrite Python modules and programs, and you will be ready to learn more about the\\nvarious Python library modules described in The Python Standard Library.\\n The Python interpreter and the extensive standard library are freely available\\nin source or binary form for all major platforms from the Python web site,\\nhttps://www.python.org/, and may be freely distributed. The same site also\\ncontains distributions of and pointers to many free third party Python modules,\\nprograms and tools, and additional documentation.\\n Whetting Your Appetite\\nThis Page\\nNavigation\\nThe Python Tutorial¶\\nPython is an easy to learn, powerful programming language. Python’s elegant syntax and dynamic typing,\\ntogether with its interpreted nature, make it an ideal language for scripting\\nand rapid application development in many areas on most platforms.\\n'}, {'url': 'https://www.w3schools.com/python/', 'content': 'Tutorials\\nHTML and CSS\\nData Analytics\\nWeb Building\\nJavaScript\\nWeb Building\\nBackend\\nData Analytics\\nWeb Building\\nExercises\\nHTML and CSS\\nData Analytics\\nJavaScript\\nBackend\\nData Analytics\\nCertificates\\nHTML and CSS\\nData Analytics\\nPrograms\\nJavaScript\\nPrograms\\nPrograms\\nBackend\\nData Analytics\\nAll Our Services\\nW3Schools offers a wide range of services and products for beginners and professionals,\\nhelping millions of people everyday to learn and master new skills.\\n Python Reference\\nYou will also find complete function and method references:\\nReference Overview\\nBuilt-in Functions\\nString Methods\\nList/Array Methods\\nDictionary Methods\\nTuple Methods\\nSet Methods\\nFile Methods\\nPython Keywords\\nPython Exceptions\\nPython Glossary\\nRandom Module\\nRequests Module\\nMath Module\\nCMath Module\\nDownload Python\\nDownload Python from the official Python web site:\\nhttps://python.org\\nKickstart your career\\nGet certified by completing the\\ncourse\\nReport Error\\nIf you want to report an error, or if you want to make a suggestion, do not hesitate to send us an e-mail:\\nhelp@w3schools.com Enjoy our free tutorials like millions of other internet users since 1999\\nExplore our selection of references covering all popular coding languages\\nCreate your own website with\\nW3Schools Spaces\\n- no setup required\\nTest your skills with different exercises\\nTest yourself with multiple choice questions\\nDocument your knowledge\\nCreate a\\nfree\\nW3Schools Account to Improve Your Learning Experience\\nTrack your learning progress at W3Schools and collect rewards\\nBecome a PRO user and unlock powerful features (ad-free, hosting, videos,..)\\n Help the lynx collect pine cones\\nFind Jobs or Hire Talent with W3Schools Pathfinder\\nJoin our newsletter and get access to exclusive content every month\\nPython Tutorial\\nFile Handling\\nPython Modules\\nPython Matplotlib\\nMachine Learning\\nPython MySQL\\nPython MongoDB\\nPython Reference\\nModule Reference\\nPython Large collection of code snippets for HTML, CSS and JavaScript\\nBuild fast and responsive sites using our free\\nW3.CSS\\nframework\\nRead long term trends of browser usage\\nTest your typing speed\\nLearn Amazon Web Services\\nUse our color picker to find different RGB, HEX and HSL colors.\\n'}, {'url': 'https://www.python.org/', 'content': 'jobs.python.org\\nLatest News\\nMore\\nUpcoming Events\\nMore\\nSuccess Stories\\nMore\\nAbridging clinical conversations using Python by Nimshi Venkat and Sandeep Konam\\nUse Python for…\\nMore\\n>>> Python Software Foundation\\nThe mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. More control flow tools in Python\\xa03\\nQuick & Easy to Learn\\nExperienced programmers in any other language can pick up Python very quickly, and beginners find the clean syntax and indentation structure easy to learn. More about lists in Python\\xa03\\nIntuitive Interpretation\\nCalculations are simple with Python, and expression syntax is straightforward: the operators +, -, * and / work as expected; parentheses () can be used for grouping. More about defining functions in Python\\xa03\\nCompound Data Types\\nLists (known as arrays in other languages) are one of the compound data types that Python understands.'}, {'url': 'https://www.python.org/about/gettingstarted/', 'content': \"You can follow this by looking at the\\nlibrary reference\\nfor a full description of Python's many libraries and the\\nlanguage reference for\\na complete (though somewhat dry) explanation of Python's syntax.\\n The Python web site\\nprovides a\\nPython Package Index\\n(also known as the Cheese Shop, a reference to the Monty Python\\nscript of that name).\\n If you do need to install Python and aren't confident about the\\ntask you can find\\na few notes on the\\nBeginnersGuide/Download\\nwiki page, but installation is unremarkable on most platforms.\\n Learning\\nBefore getting started, you may want to find out which IDEs and text\\neditors are tailored to make\\nPython editing easy, browse the list of introductory books, or look at code samples that you might find\\nhelpful.\\n Frequently Asked Questions\\nIf you have a question, it's a good idea to try the\\nFAQ, which answers the most commonly\\nasked questions about Python.\\n\"}, {'url': 'https://www.python.org/?lang=python', 'content': 'The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. Learn more. Become a Member Donate to the PSF. The official home of the Python Programming Language.'}]\n",
      "\n",
      "content='The current capital of France is Paris. It has been the capital since 1958 and is also the most populous city in the country. Paris has a rich history dating back to the 13th century and is renowned for its culture, fashion, cuisine, and art.\\n\\nThe historical capital of France, before the formation of the French Republic, was Versailles. The city was the seat of the French monarchy and the site of the famous Palace of Versailles, which remains an iconic landmark today.' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '1fae3f3e-25cb-49fd-bbf2-1f9d2dca2fb4'} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '1fae3f3e-25cb-49fd-bbf2-1f9d2dca2fb4'} id='run-42b7b431-48d4-4fb6-bc96-534c94999e28-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Set model\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\").invoke(\"Hello, how are you?\")\n",
    "print(\"--Model API Check--\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Set embeddings\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(\"--Embeddings API Check--\")\n",
    "print(query_result)\n",
    "print()\n",
    "\n",
    "# Set web search\n",
    "web_search_tool = TavilySearchResults()\n",
    "search_results = web_search_tool.invoke(\"python\")\n",
    "print(\"--Web Search API Check--\")\n",
    "print(search_results)\n",
    "print()\n",
    "\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "print(llm.invoke(\"What is the capital of France?\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "use_serverless = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, PodSpec  \n",
    "import time  \n",
    "# configure client  \n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)  \n",
    "if use_serverless:  \n",
    "    spec = ServerlessSpec(cloud='aws', region='us-east-1')  \n",
    "else:  \n",
    "    # if not using a starter index, you should specify a pod_type too  \n",
    "    spec = PodSpec()  \n",
    "# check for and delete index if already exists  \n",
    "index_name = 'pdf-chatter'  \n",
    "if index_name in pc.list_indexes().names():  \n",
    "    pc.delete_index(index_name)  \n",
    "# create a new index  \n",
    "pc.create_index(  \n",
    "    index_name,  \n",
    "    dimension=1024,  # dimensionality of text-embedding-ada-002  \n",
    "    metric='cosine',  \n",
    "    spec=spec  \n",
    ")  \n",
    "# wait for index to be initialized  \n",
    "while not pc.describe_index(index_name).status['ready']:  \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up vector store of the given pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a PDF document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf = \"research.pdf\"\n",
    "pdf_description = \"This is a research paper on the topic of AI.\"\n",
    "\n",
    "loader = PyPDFLoader(pdf, extract_images=True)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the document into chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Backprop-Free Reinforcement Learning with Active Neural Generative Coding\\nAlexander G. Ororbia1, Ankur Mali2\\n1Rochester Institute of Technology\\n2The Pennsylvania State University\\nago@cs.rit.edu, aam35@psu.edu\\nAbstract\\nIn humans, perceptual awareness facilitates the fast recogni-\\ntion and extraction of information from sensory input. This\\nawareness largely depends on how the human agent interacts\\nwith the environment. In this work, we propose active neural\\ngenerative coding, a computational framework for learning\\naction-driven generative models without backpropagation of\\nerrors (backprop) in dynamic environments. Speciﬁcally, we\\ndevelop an intelligent agent that operates even with sparse re-\\nwards, drawing inspiration from the cognitive theory of plan-\\nning as inference. We demonstrate on several simple control\\nproblems that our framework performs competitively with\\ndeep Q-learning. The robust performance of our agent offers\\npromising evidence that a backprop-free approach for neural\\ninference and learning can drive goal-directed behavior.\\nIntroduction\\nManipulating one’s environment in the effort to understand\\nit is an essential ingredient of learning in humans (Spiel-\\nberger and Starr 1994; Berlyne 1966). In cognitive neuro-\\nscience, behavioral and neurobiological evidence indicates a\\ndistinction between goal-directed and habitual action selec-\\ntion in reward-based decision-making. With respect to ha-\\nbitual action selection, or actions taken based on situation-\\nresponse associations, ample evidence exists to support the\\ntemporal-difference (TD) account from reinforcement learn-\\ning. In this account, the neurotransmitter dopamine creates\\nan error signal based on reward prediction to drive (state) up-\\ndates in the corpus striatum, a particular neuronal region in\\nthe basal ganglia that affects an agent’s choice of action. In\\ncontrast, goal-directed action requires prospective planning\\nwhere actions are taken based on predictions of their future\\npotential outcomes (Niv 2009; Solway and Botvinick 2012).\\nPlanning-as-inference (PAI) (Botvinick and Toussaint 2012)\\nattempts to account for goal-directed behavior by casting it\\nas a problem of probabilistic inference where an agent ma-\\nnipulates an internal model that estimates the probability of\\npotential action-outcome-reward sequences.\\nOne important, emerging theoretical framework for PAI is\\nthat of active inference (Friston, Mattout, and Kilner 2011;\\nTschantz, Seth, and Buckley 2020), which posits that bio-\\nlogical agents learn a probabilistic generative model by in-\\nCopyright © 2022, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.teracting with their world, adjusting the internal states of this\\nmodel to account for the evidence that they acquire from\\ntheir environment. This scheme uniﬁes perception, action,\\nand learning in adaptive systems by framing them as pro-\\ncesses that result from approximate Bayesian inference, ele-\\ngantly tackling the exploration-exploitation trade-off inher-\\nent to organism survival. The emergence of this framework\\nis timely – in reinforcement learning (RL) research, despite\\nthe recent successes afforded by artiﬁcial neural networks\\n(ANNs) (Mnih et al. 2013; Silver et al. 2018), most models\\nrequire exorbitant quantities of data to train well, struggling\\nto learn tasks as efﬁciently as humans and animals (Arulku-\\nmaran et al. 2017). As a result, a key challenge is how to\\ndesign RL methods that successfully resolve environmental\\nuncertainty and complexity given limited resources and data.\\nModel-based RL, explored in statistical learning research\\nthrough world (Ha and Schmidhuber 2018) or dynamics\\nmodels (Sutton 1990), offers a promising means of tack-\\nling this challenge and active inference provides a promising\\npath towards instantiating it in a powerful yet neurocogni-\\ntively meaningful way (Tschantz et al. 2020b).\\nAlthough PAI and active inference offer an excellent story', metadata={'source': 'research.pdf', 'page': 0}),\n",
       " Document(page_content='path towards instantiating it in a powerful yet neurocogni-\\ntively meaningful way (Tschantz et al. 2020b).\\nAlthough PAI and active inference offer an excellent story\\nfor biological system behavior and a promising model-based\\nRL setup, most computational implementations are formu-\\nlated with explainability in mind (favoring meaningfully la-\\nbeled albeit low-dimensional, discrete state/action spaces)\\nyet in the form of complex probabilistic graphical models\\nthat do not scale easily (Friston et al. 2015, 2017a,b, 2018).\\nIn response, effort has been made to scale active inference by\\nusing deep ANNs (Ueltzh ¨offer 2018; Tschantz et al. 2020a)\\ntrained by the popular backpropagation of errors (backprop)\\n(Rumelhart, Hinton, and Williams 1986). While ANNs rep-\\nresent a powerful step in the right direction, one common\\ncriticism of using them within the normative framework\\nof RL is that they have little biological relevance despite\\ntheir conceptual value (Lake et al. 2017; Zador 2019). Im-\\nportantly, from a practical point-of-view, they also suffer\\nfrom practical issues related to their backprop-centric de-\\nsign (Ororbia and Mali 2019). This raises the question: can a\\nbiologically-motivated alternative to backprop-based ANNs\\nalso facilitate reinforcement learning through active infer-\\nence in a scalable way? In this paper, motivated by the fact\\nthat animals and humans solve the RL problem, we develop\\none alternative that positively answers this question. As a re-\\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\\n29', metadata={'source': 'research.pdf', 'page': 0}),\n",
       " Document(page_content='atot+1External World Internal Model\\nrtot\\nCorrectGenerateFigure 1: An ANGC agent predicts the world, acts to manip-\\nulate it, and then corrects itself given observations/rewards.\\nsult, the neural agent we propose represents a promising step\\nforward towards better modeling the approximations that bi-\\nological neural circuitry implements when facing real-world\\nresource constraints and limitations, creating the potential\\nfor developing new theoretical insights. Such insights will\\nallow us to design agents better capable of dealing with con-\\ntinuous, noisy sensory patterns (Niv 2009).\\nWhile many backprop-alternative (backprop-free) al-\\ngorithms have recently been proposed (Movellan 1991;\\nO’Reilly 1996; Lee et al. 2015; Lillicrap et al. 2016; Scellier\\nand Bengio 2017; Guerguiev, Lillicrap, and Richards 2017;\\nWhittington and Bogacz 2017; Ororbia and Mali 2019), few\\nhave been investigated outside the context of supervised\\nlearning, with some notable exceptions in sequence (Wise-\\nman et al. 2017; Ororbia et al. 2020a; Manchev and Spratling\\n2020) and generative modeling (Ororbia and Kifer 2020). In\\nthe realm of RL, aside from neuro-evolutionary approaches\\n(Such et al. 2017; Heidrich-Meisner and Igel 2009) or meth-\\nods that build on top of them (Najarro and Risi 2020), the\\ndearth of work is more prescient and our intent is to close\\nthis gap by providing a backprop-free approach to inference\\nand learning, which we call active neural generative cod-\\ning(ANGC), to drive goal-oriented agents. In our system,\\nwe demonstrate how a scalable, biologically-plausible infer-\\nence and learning process, grounded in the theory of pre-\\ndictive processing (Friston 2005; Clark 2015), can lead to\\nadaptive, self-motivated behavior in the effort to balance the\\nexploration-exploitation trade-off in RL. One key element\\nto consider is that ANGC offers robustness in settings with\\nsparse rewards which other backprop-free methods such as\\nneuroevolution (Such et al. 2017; Heidrich-Meisner and Igel\\n2009) struggle with.1To evaluate ANGC’s efﬁcacy, we im-\\nplement an agent structure tasked with solving control prob-\\nlems often experimented with in RL and compare perfor-\\nmance against several backprop-based approaches.\\nActive Neural Generative Coding\\nTo specify our proposed ANGC agent, the high-level intu-\\nition of which is illustrated in Figure 1, we start by deﬁning\\nof the fundamental building block used to construct it, i.e.,\\n1It is difﬁcult to determine a strong encoding scheme as well as\\nan effective breeding strategy for the underlying genetic algorithm.\\nSuch design choices play a large role in the success of the approach.the neural generative coding circuit. Speciﬁcally, we exam-\\nine its neural dynamics (for ﬁguring out hidden layer values\\ngiven inputs and outputs) and its synaptic weight updates.\\nThe Neural Generative Coding Circuit\\nNeural generative coding (NGC) is a recently developed\\nframework (Ororbia and Kifer 2020) that generalizes clas-\\nsical ideas in predictive processing (Rao and Ballard 1999;\\nClark 2015) to the construction of scalable neural models\\nthat model and predict both static and temporal patterns\\n(Ororbia et al. 2020a; Ororbia and Kifer 2020). An NGC\\nmodel is composed of L+ 1 layers of stateful neurons\\nN0;N1;···;NLthat are engaged in a process of never-\\nending guess-then-correct, where N`containsJ`neurons\\n(each neuron has a latent state value represented by a scalar).\\nThe combined latent state of the neurons in N`is repre-\\nsented by the vector z`∈RJ`×1(initially z`=0in the\\npresence of a new data pattern). Generally, an NGC model’s\\nbottom-most layer N0is clamped to a sensory pattern ex-\\ntracted from the environment. However, in this work, we de-\\nsign a model that clamps both its top-most layer NLand\\nbottom-most layer N0to particular sensory variables, i.e.,\\nzL=xiandz0=xo, allowing the agent to process streams\\nof data (xi;xo)where xi∈RJL×1andxo∈RJ0×1.\\nSpeciﬁcally, in an NGC model, layer N`+1attempts to', metadata={'source': 'research.pdf', 'page': 1}),\n",
       " Document(page_content='zL=xiandz0=xo, allowing the agent to process streams\\nof data (xi;xo)where xi∈RJL×1andxo∈RJ0×1.\\nSpeciﬁcally, in an NGC model, layer N`+1attempts to\\nguess the current post-activity values of layer N`, i.e.,\\n\\x1e`(z`), by generating a prediction vector \\x16 z`using a ma-\\ntrix of forward synaptic weights W`+1∈RJ`×J`+1. The\\nprediction vector is then compared against the target ac-\\ntivity by a corresponding set of error neurons e`which\\nsimply perform a direct mismatch calculation as follows:\\ne`=1\\n\\x0ce(\\x1e`(z`)−\\x16 z`).2This error signal is ﬁnally transmit-\\nted back to the layer that made the prediction \\x16 z`through a\\ncomplementary matrix of error synapses E`+1∈RJ`+1×J`.\\nGiven the description above, the set of equations that charac-\\nterize the NGC neural circuit and its key computations are:\\n\\x16 z`=g`\\x10\\nW`+1·\\x1e`+1(z`+1)\\x11\\n;e`=1\\n2\\x0ce(\\x1e`(z`)−\\x16 z`)\\n(1)\\nz`+1←z`+1+\\x0c\\x10leakz}|{\\n−\\rvz`+1+pressurez}|{\\nd`+1+lateral termz}|{\\n#(z`+1))\\x11\\n(2)\\nwhere d`+1=−e`+1+ (E`+1·e`)\\nwhere·indicates matrix/vector multiplication and \\x1e`+1and\\ng`are element-wise activation functions, e.g., the hyperbolic\\ntangenttanh(v) = (exp(2v)−1)=(exp(2v) + 1) or the lin-\\near rectiﬁer \\x1e`(v) =max(0;v). In this paper, we set g`as\\nthe identity, i.e., g`(v) =v, for all layers. In Equation 2, the\\ncoefﬁcient that weights the correction applied to state layer\\n`+1is determined by the formula \\x0c=1\\n\\x1cwhere\\x1cis the inte-\\ngration time constant in the order of milliseconds. The leak\\nvariable−\\rvz`decays the state value over time (\\r vis a posi-\\ntive coefﬁcient to control its strength). #(z`)is lateral excita-\\n2One may replace1\\n2\\x0cewith\\x06−1, i.e., a learnable matrix that\\napplies precision-weighting to the error units, as in (Ororbia and\\nKifer 2020). We defer using this scheme for future work.\\n30', metadata={'source': 'research.pdf', 'page': 1}),\n",
       " Document(page_content='z2\\nz1\\nz0z1\\nz0e1\\ne0\\nxx=\\n=i\\no\\not\\n \\nttot+1\\nrt\\nrterti\\nEnvironmentPolicyForward Model\\nat\\nat\\nFigure 2: The NGC circuit (left) and the high-level ANGC architecture, with both a controller and generator, (right). Green\\ndiamonds represent error neurons, empty rectangles represent state neurons, solid arrows represent individual synapses, dashed\\narrows represent direct copying of information, open circles indicate excitatory signals, and ﬁlled squares indicate inhibitory\\nsignals. Note that tt, which is enclosed in a rounded blue box, entails a more intricate calculation (see Equation 6).\\ntion/inhibition term, which is a function that creates compe-\\ntition patterns among the neurons inside of layer N`(Oror-\\nbia and Kifer 2020) – in this paper we set #(z`) = 0 since its\\neffect is not needed for this study. Upon encountering data\\n(xi;xo), the model’s top and bottom layers are clamped, i.e.,\\nzL=xiandz0=xo, and Equations 1-2 are run Ktimes\\nin order to search for activity values {z1;···;zL−1}(see\\nINFER in the pseudocode provided in the Appendix3).\\nAfter the internal activities have been found, the synap-\\ntic weights may be adjusted using a modulated local error\\nHebbian rule adapted from local representation alignment\\n(LRA) (Ororbia and Mali 2019; Ororbia et al. 2020b):\\n\\x01W`=e`·(\\x1e`(z`+1))T⊗M`\\nW (3)\\n\\x01E`=\\re(W`\\nt−W`\\nt−1)⊗M`\\nE (4)\\nwhere\\recontrols the time-scale at which the error synapses\\nare adjusted (usually values in the range of [0:9;1:0]are\\nused). The dynamic modulation factors M`\\nWandM`\\nEhelp\\nto stabilize learning in the face of non-stationary streams and\\nare based on insights related to nonlinear synaptic dynamics\\n(see Appendix for a full treatment of these factors). Once\\nthe weight adjustments have been computed, an update rule\\nsuch as stochastic gradient ascent, Adam (Kingma and Ba\\n2014), or RMSprop (Tieleman and Hinton 2012) can be used\\n(see U PDATE WEIGHTS in Algorithm CC, Appendix).\\nThe online objective that an NGC model attempts to min-\\nimize is known as total discrepancy (TotD) (Ororbia et al.\\n2017), from which the error neuron, state update expres-\\nsions, and local synaptic adjustments may be derived (Oror-\\nbia et al. 2020b; Ororbia and Kifer 2020). The (TotD) objec-\\ntive, which could also be interpreted as a form of free energy\\n(Friston 2009) specialized for stateful neural models that uti-\\nlize arbitrary forward and error synaptic pathways (Ororbia\\net al. 2020b), can be expressed in many forms including the\\nlinear combination of local density functions (Ororbia and\\nKifer 2020) or the summation of distance functions (Oror-\\n3Appendix at: https://arxiv.org/abs/2107.07046.bia et al. 2020a). For this study, the form of TotD used is:\\nL(\\x02) =L−1X\\n`=0L(z`;\\x16 z`) =L−1X\\n`=01\\n\\x0ce||\\x1e`(z`)−\\x16 z`||2\\n2.\\nAlgorithm CC (see Appendix), puts all of the equations\\nand relevant details presented so far together to describe how\\nNGC processes (xi;xo)from a data stream. We note here\\nthat the algorithm breaks down the processing into three rou-\\ntines – I NFER (◦), UPDATE WEIGHTS (◦), and P ROJECT (◦).\\nINFER (◦)is simply the K-step process described earlier\\nto ﬁnd reasonable values of the latent state activities given\\nclamped data and U PDATE WEIGHTS (◦)is the complemen-\\ntary procedure used to adjust the synaptic weight parameters\\nonce state activities have been found after using I NFER (◦).\\nPROJECT (◦)is a special function that speciﬁcally clamps\\ndataxito the top-most layer and projects this information\\ndirectly through the underlying directed graph deﬁned by the\\nNGC architecture – this routine is essentially a variant of the\\nancestral sampling procedure deﬁned in (Ororbia and Kifer\\n2020) but accepts a clamped input pattern instead of samples\\ndrawn from a prior distribution. Figure 2 (left) graphically\\ndepicts a three layer NGC model with 2neurons per layer.\\nGeneralizing to Active Neural Coding\\nGiven the deﬁnition of the NGC block, we turn our atten-\\ntion to its generalization that incorporates actions, i.e., active', metadata={'source': 'research.pdf', 'page': 2}),\n",
       " Document(page_content='Generalizing to Active Neural Coding\\nGiven the deﬁnition of the NGC block, we turn our atten-\\ntion to its generalization that incorporates actions, i.e., active\\nNGC (ANGC). ANGC is built on the premise that an agent\\nadapts to its environment by balancing a trade-off between\\n(at least) two key quantities – surprisal and preference. This\\nmeans that our agent is constantly tracking a measurement of\\nhow surprising the observations it encounters are at a given\\ntime step (which drives exploration) as well as a measure-\\nment of its progress towards a goal. In effect, maximizing\\nthe sum of these two terms means that the agent will seek\\nobservations that are most “suprising” (which yield the most\\ninformation when attempting to reduce uncertainty) while\\nattempting to reduce its distance to a goal state (which max-\\nimizes the discounted long-term future reward). Formally,\\n31', metadata={'source': 'research.pdf', 'page': 2}),\n",
       " Document(page_content='this means that an ANGC agent will maximize:\\nrt=\\x0bere\\nt+\\x0biri\\nt=rep\\nt+rin\\nt (5)\\nwhich is a reward signal that can be decomposed into an\\nepistemic (or exploration/information maximizing) signal\\nrep\\ntand an instrumental (or goal-oriented) signal rin\\nt. Each\\ncomponent signal is a product of an importance factor, \\x0be\\nfor the epistemic term and \\x0bifor the instrumental term, and a\\nraw internal signal produced either by the generative model\\n(re\\ntto driverep\\nt) or an external goal-directing signal (ri\\ntto\\ndriverin\\nt). Although we chose to interpret and represent the\\nactive inference view of the exploration-exploitation trade-\\noff as (dopamine) scalars, our instrumental signal is not lim-\\nited to this scheme and could incorporate an encoding of\\nmore complex functions such as (prior) distribution func-\\ntions over (goal) states (see Appendix).\\nAs indicated by the architecture diagram in Figure 2\\n(right), the implementation of our ANGC agent in this pa-\\nper is a coupling of two NGC circuits – the generator (or\\ndynamic generative model), which is responsible for pro-\\nducing the epistemic term rep\\nt, and the controller, which is\\nresponsible for choosing the actions such that the full reward\\nrt, which includes the instrumental term rin\\nt, is maximized.\\nThe NGC Generator (Forward Model) Once the gen-\\nerator’s top-most latent state is clamped to the current D-\\ndimensional observation ot∈ RD×1and the 1-of-A en-\\ncoding of the controller’s currently chosen action at(out\\nofApossible actions), i.e., at∈{0;1}A×1, the generator\\nattempts to predict the value of the next observation of the\\nenvironment ot+1. Using the routine I NFER (Algorithm CC,\\nAppendix), the generator, with parameters \\x02g, searches for a\\ngood set of latent state activities to explain output xo=ot+1\\ngiven input xi= [at;ot+1]where [·;·]indicates the vector\\nconcatenation of otandat. Once latent activities have been\\nfound, the generator then updates its synapses via routine\\nUPDATE WEIGHTS (Algorithm CC, Appendix).\\nThe generator plays a key role in that it drives the ex-\\nploration conducted by an ANGC agent. Speciﬁcally, as the\\ngenerator progressively learns to how to synthesize future\\nobservations, the current activities of its error neurons em-\\nbedded at each layer, i.e., E={e0;e1;···;eL}, are used to\\nproduce an epistemic modulation term. Formally, this means\\nthat the exploration signal is calculated as re\\nt=P\\n`||e`||2\\n2\\nwhich is the result of summing across layers and across each\\nerror neuron vector’s respective dimensions.4The epistemic\\ntermrep\\nt=\\x0bere\\ntis then combined with an instrumental term\\nrin\\nt=\\x0biri\\nt, i.e., the scalar signal produced externally (by the\\nenvironment or another neural system), to guide the agent\\ntowards a goal state(s), via Equation 5. The ﬁnal rtis then\\nsubsequently used to adapt the controller (described next).\\nThe NGC Controller (Policy Model) With its top-most\\nlatent state clamped to the tth observation, i.e., xi=ot, the\\ncontroller, with parameters \\x02c, will generate a prediction of\\nthe full reward signal rt. Speciﬁcally, at time t, given a target\\nscalar (produced by the environment and the generator), the\\n4Observe that re\\ntis the generator’s TotD, i.e., re\\nt=L(\\x02g).controller will also infer a suitable set of latent activities us-\\ning the I NFER routine deﬁned in Algorithm CC (Appendix).\\nSince the NGC controller’s output layer will estimate a\\npotential reward signal for each possible discrete action that\\nthe agent could take (which is typical in many modern Q-\\nlearning setups), we must ﬁrst compose the target activity\\nttfor the output nodes once the scalar value rtis obtained.\\nThis is done by ﬁrst encoding the action as a 1-of-A vector\\nat(this is what done by the TOONEHOTfunction call in Al-\\ngorithm 1), computing the boot-strap estimate of the future\\ndiscounted reward dt+1=PROJECT (ot+1;\\x02c), and ﬁnally\\nchecking if the next observation is a terminal. Speciﬁcally,\\nthe target vector is computed according to the following:\\ntt=ttat+ (1−at)⊗PROJECT (ot;\\x02c) (6)', metadata={'source': 'research.pdf', 'page': 3}),\n",
       " Document(page_content='checking if the next observation is a terminal. Speciﬁcally,\\nthe target vector is computed according to the following:\\ntt=ttat+ (1−at)⊗PROJECT (ot;\\x02c) (6)\\nwhere the target scalar ttis created according to the follow-\\ning logical expression:\\n(otis terminal→tt=rt)∧(otis not terminal→t′\\nt)(7)\\nwheret′\\nt=rt+\\rmax\\naPROJECT (ot+1;\\x02c)).\\nOnce tthas been prepared, the controller is run to ﬁnd its\\nlatent activities for otandttusing I NFER and calculates\\nits local weight updates via the U PDATE WEIGHTS routine\\n(Algorithm CC, Appendix). Furthermore, observe that the\\nsecond sub-expression in Equation 7 involves re-using the\\ncontroller to estimate the (reward) value of the future obser-\\nvation, i.e.,\\rmaxaPROJECT (ot+1;\\x02c)term. This term can\\nbe replaced with a proxy term \\rmaxaPROJECT (ot+1;b\\x02c)\\nto implement the target network stability mechanism pro-\\nposed in (Mnih et al. 2015), where b\\x02care the parameters of\\na “target controller”, initialized to be the values of \\x02cat the\\nstart of the simulation and updated every Ctransitions or by\\nPolyak averaging b\\x02c=\\x1cc\\x02c+ (1−\\x1cc)b\\x02c.\\nThe ANGC Agent: Putting It All Together At a high\\nlevel, the ANGC operates, given observation ot, according\\nto the following steps: 1) the NGC controller takes in otand\\nuses it to produce a discrete action, 2) the ANGC agent next\\nreceives observation ot+1from the environment, i.e., the re-\\nsult of its action, 3) the NGC generator runs the dynamics in\\nEquations 1-2 to ﬁnd a set of hidden neural activity values\\nthat allow a mapping from [a;ot]toot+1and then updates\\nits own speciﬁc synapses using Equations 3-4, 4) the reward\\nrtis computed using the extrinsic/problem-speciﬁc reward\\nplus the epistemic signal (produced by summing the layer-\\nwise errors inside the generator, i.e., total discrepancy), 5)\\nthe NGC controller then runs the dynamics in Equations 1-\\n2 to ﬁnd a set of hidden neural activity values that allow a\\nmapping from ottott(which contains rt) and then updates\\nits synapses via Equations 3-4, and, ﬁnally, 6) the ANGC\\nagent transitions to ot+1and moves back to step 1.\\nThe above step-by-step process shows that the NGC gen-\\nerator (forward model) drives information-seeking behavior\\n(facilitating exploration better than that of random epsilon-\\ngreedy), allowing the ANGC agent to evaluate if an incom-\\ning state will allow for a signiﬁcant reduction in uncertainty\\nabout the environment. The NGC controller (policy) is re-\\nsponsible for estimating future discounted rewards, balanc-\\ning the seeking of a goal state (since the instrumental term\\n32', metadata={'source': 'research.pdf', 'page': 3}),\n",
       " Document(page_content='Algorithm 1: The ANGC total discrepancy process under an environment for Eepisodes (of maximum length T).\\nInput: environment S, controller \\x02c, generator \\x02g, deque memoryM, and constants E,T,\\x0be,\\x0bi,\\x0fdecay ,\\x0f,\\r\\nfunction SIMULATE PROCESS (S;E;T; \\x02c;\\x02g;M;\\x0be;\\x0bi;\\x0f;\\x0fdecay )\\nre\\nmax= 1\\nfore= 1toEdo\\not←o0fromS .Get initial state/observation from environment\\nfort= 1toTdo\\n==Sample action ataccording to an \\x0f-greedy policy\\ndt=PROJECT (ot;\\x02c),p∼U(0;1)\\x10\\np<\\x0f→at∼Ud(1;A)\\x11\\n∧\\x10\\np≥\\x0f→at= arg max adt\\x11\\n,at=TOONEHOT(at)\\n==Get next state/observation from environment & compute component reward signals\\n(ri\\nt;ot+1)←S(at),(\\x03;E) =INFER ([at;ot];ot+1;\\x02g)\\nre\\nt=P\\n`||E[`]||2\\n2,re\\nmax= max(re\\nt;re\\nmax),re\\nt←re\\nt\\nre\\nmax,rt=\\x0bere\\nt+\\x0biri\\nt\\n==Store transition and update weights from samples in memory\\nStore (ot;at;rt;ot+1)inM\\n(oj;aj;rj;oj+1)∼M .Sample mini-batch of transitions from memory\\ntj=\\x1arj ifojis terminal\\nrj+\\rmaxaPROJECT (oj+1;\\x02c)otherwise\\naj=TOONEHOT(aj),tj=tjaj+ (1−aj)⊗PROJECT (oj;\\x02c)\\n(\\x03c;Ec) =INFER (oj;tj;\\x02c),\\x02c←UPDATE WEIGHTS (\\x03c;Ec;\\x02c) .Update controller \\x02c\\n(\\x03g;Eg) =INFER ([aj;oj];oj+1;\\x02g),\\x02g←UPDATE WEIGHTS (\\x03g;Eg;\\x02g) .Update generator \\x02g\\n\\x0f←max(0:05;\\x0f·\\x0fdecay)\\nrepresents the “desire” to solve the problem) with the search\\nfor states that will give it the most information about its envi-\\nronment. The controller keeps the agent focused on ﬁnding a\\ngoal state(s) and reinforcing discovered sequences of actions\\nthat lead to these goal states (exploitation) while the genera-\\ntor forces the agent to parsimoniously explore its world and\\nseek elements that it knows least about but will likely help\\nin ﬁnding goal states – this reduces the number of episodes\\nand/or sampled states needed to uncover useful policies.\\nGiven that ANGC is inspired by active inference (Fris-\\nton, Mattout, and Kilner 2011), the intuition behind our ap-\\nproach is that an agent reduces the divergence between its\\nmodel of the world and the actual world by either: 1) chang-\\ning its internal model (the generator) so that it better aligns\\nwith sampled observations (which is why it seeks states with\\nhigh epistemic/total discrepancy values), or 2) changing its\\nobservations such that they align with its internal model\\nthrough action, i.e., this is done through the controller track-\\ning its problem-speciﬁc performance either through extrin-\\nsic reward values or other functions, i.e., prior preferences\\n(Tschantz, Seth, and Buckley 2020). The ANGC agent’s bal-\\nancing act between ﬁnding goal states with better exploring\\nits environment strongly relates to the rise of computational\\ncuriosity in the RL literature (Pathak et al. 2017) – since we\\nfocus on using problem-speciﬁc rewards (instead of craft-\\ning prior preference distributions as in (Friston, Daunizeau,\\nand Kiebel 2009; Tschantz, Seth, and Buckley 2020)) our\\nagent’s instrumental term (in Equation 5) is akin to extrinsic\\nreward and our epistemic term is similar in spirit to intrinsic\\n“curiosity” (Oudeyer 2018; Burda et al. 2018a) (error-based\\ncuriosity). Although our curiosity term is the total discrep-\\nancy of the NGC generator, this term connects ANGC to thecuriosity-based mechanisms and exploration bonuses (Wu\\nand Tian 2016) used to facilitate efﬁcient extraction of ro-\\nbust goal-state seeking policies. Furthermore, the generator\\ncomponent of the ANGC agent connects our work with the\\nrecently growing interest in model-based RL where world\\nmodels are integrated into the agent-environment interac-\\ntion process, e.g., plan2explore (Sekar et al. 2020), dreamer\\n(Hafner et al. 2019), etc. In a sense, one could view our\\nANGC as a simple, neurobiologically-plausible instantia-\\ntion of these kinds of model-based RL approaches, offering\\na means to train similar systems without backprop. Many\\nother bio-inspired algorithms, such as equilibrium propa-\\ngation (Scellier and Bengio 2017), do not scale easily to\\nRL problems due to expensive inference phases (whereas\\nNGC’s inference is faster – see Appendix for details).', metadata={'source': 'research.pdf', 'page': 4}),\n",
       " Document(page_content='gation (Scellier and Bengio 2017), do not scale easily to\\nRL problems due to expensive inference phases (whereas\\nNGC’s inference is faster – see Appendix for details).\\nIn essence, the proposed ANGC framework prescribes the\\njoint interaction of the controller and generator modules de-\\nscribed above. At each time step, the agent, given observa-\\ntionot∈RD×1(which could contain continuous or dis-\\ncrete variables), is to perform a discrete action at5and re-\\nceive from its environment the result of its action, i.e., ob-\\nservation ot+1and possibly an external reward signal rep\\nt.\\nThe controller is responsible for deciding which action to\\ntake next while the generator actively attempts to guess the\\n(next) state of the agent’s environment. Upon taking an ac-\\ntionat, the generator’s prediction is corrected using the val-\\nues within the sensory sample drawn from the environment,\\nallowing it to iteratively craft a compressed internal impres-\\nsion of the agent’s world. The inability of the generator to\\n5We focus on discrete actions in this study and leave general-\\nization to continuous actions for future work.\\n33', metadata={'source': 'research.pdf', 'page': 4}),\n",
       " Document(page_content='accurately predict the incoming sensory sample ot+1pro-\\nvides the agent with a strong guide to its exploration of the\\nenvironment, reducing its (long-term) surprisal and improv-\\ning the controller’s ability to extract an effective policy/plan.\\nThe complete ANGC agent is speciﬁed in Algorithm 16\\nand depicted in Figure 2 (right). Note that Algorithm 1\\nimplements the full simulation of ANGC’s inference and\\nsynaptic adjustment over an E-episode long stream (each\\nepisode is at most Tsteps long – where Tcould vary with\\ntime). In addition to the target controller modiﬁcation de-\\nscribed earlier, we integrate experience replay memory M\\n(O’Neill et al. 2010; Mnih et al. 2015) (implemented as a\\nring buffer with mini-batches sampled, uniformly at random,\\nfrom stored transitions). This stabilizes the learning process\\nby removing correlations in the observation sequence.\\nExperiments\\nThe performance of the ANGC agent is evaluated on three\\ncontrol problems commonly used in reinforcement learning\\n(RL) and one simulation in robotic control. Speciﬁcally, we\\ncompare ANGC to a random agent (where actions are taken\\nat each step uniformly at random), a deep Q-network (DQN)\\n(Mnih et al. 2015), the intrinsic curiosity module (ICM)\\n(Pathak et al. 2017), and another powerful intrinsic curios-\\nity baseline known as random network distillation (RnD)\\n(Burda et al. 2018b) on: 1) the inverted pendulum (cartpole)\\nproblem, 2) the mountain car problem 3) the lunar lander\\nproblem, and 4) the robot reaching problem. Details related\\nto each control problem are provided in the Appendix.\\nANGC Agent Setup: For all of the ANGC agents across\\nall trials, we used ﬁxed conﬁgurations of meta-parameters\\nfor each control task. We provide the key values cho-\\nsen (based on preliminary experimentation) for the meta-\\nparameters for all ANGC models in the Appendix.\\nFor all ANGC agents, \\x0be=\\x0bi= 1:0was used as the im-\\nportance factors for both the epistemic and instrumental sig-\\nnals. Both the controller and generative model were trained\\nusing a single, shared experience replay buffer with a maxi-\\nmum capacity of Nmem transitions from which mini-batches\\nofNbatch transitions were sampled in order to compute pa-\\nrameter updates at any single time-step of each simulation.\\nEach agent also uses an epsilon(\\x0f)-greedy policy where \\x0f\\nwas decayed at the end of each episode according to the rule:\\n\\x0f←min(0:05;\\x0f∗\\x0fdecay)(starting\\x0f= 1at a trial’s start).\\nIn addition, we experiment with an ablated form of our\\nANGC agent, i.e., Instr-ANGC, where the generator/for-\\nward model has been removed. This means that the Instr-\\nANGC only uses the extrinsic reward signal, allowing for a\\ncloser examination of what happens if the normal backprop-\\nbased neural model was just replaced with our NGC circuit.\\nBaseline Agent Setups: For the DQN, ICM, and RnD\\nagents, we initially start with 90% exploration and 10% ex-\\nploitation (\\x0f = 0:9) and eventually begin decaying until the\\ncondition of 10% exploration is reached, i.e., 90% exploita-\\ntion (\\x0f = 0:1). The discount factor was tuned in the range of\\n6E[`]means “retrieve the `th item in E′′.\\r= [0:91;0:99]. The linear rectiﬁer was used as the activa-\\ntion function and Adam was used to update weight values,\\nexcept for ICM, where AdamW was found to yield more sta-\\nble updates. For all models, each W`was initialized accord-\\ning to a centered Gaussian scaled byp\\n2:0=(J`−1+J`).\\nThe replay buffer size, the learning rate, the hidden dimen-\\nsions, and number of layers were tuned – hidden layer sizes\\nwere selected from the range of [32;512] and the number of\\nlayers was chosen from the set [1;2;3]. In the Appendix, we\\nprovide best conﬁgurations used for each model.\\nResults: In Figures 3 and 4, we visualize the accumulated\\nreward as a function of the episode count (over the ﬁrst 1000\\nepisodes, see Appendix for expanded results), smoothing out\\nthe curves by plotting the moving average reward as a func-\\ntion of the episode count, i.e., \\x16t= 0:1rt+ 0:9\\x16t−1. Re-', metadata={'source': 'research.pdf', 'page': 5}),\n",
       " Document(page_content='episodes, see Appendix for expanded results), smoothing out\\nthe curves by plotting the moving average reward as a func-\\ntion of the episode count, i.e., \\x16t= 0:1rt+ 0:9\\x16t−1. Re-\\nsults are averaged over 10trials and the plots present both\\nthe mean (darker color central curve) and standard deviation\\n(lighter color envelope). In each plot, a dash-dotted horizon-\\ntal line depicts the threshold for fully solving each task.\\nIt is immediately apparent from our reward plots, across\\nall four benchmarks, that the ANGC agent is notably com-\\npetitive with backprop-based, intrinsic curiosity models\\n(ICM, RnD), extracting a better policy than models that\\ndo not incorporate intelligent exploration (the DQN). This\\nhighlights the value of our ANGC framework for design-\\ning agents – for each of the benchmarks we investigate,\\nfewer episodes are required by ANGC agents to generalize\\nwell and even ultimately solve a given control problem (as\\nindicated by their ability to reach each problem’s solution\\nthreshold). Furthermore, ANGC offers stable performance\\nin general, where models like the RnD sometimes do not\\n(as on the mountain car problem), and is notably quite com-\\npetitive with ICM in general, even outperforming it on the\\nmore complex robotic arm control task. We also note that the\\nDQN reaches its goal quickly in some cases but struggles to\\nmaintain itself at the solution threshold, ﬂuctuating around\\nthat range (requiring more episodes to fully stabilize).\\nCrucially, observe that the ANGC agent is capable of\\neffectively tackling control problems involving extremely\\nsparse rewards (or nearly non-existent reward signals) as in-\\ndicated by its early strong performance on the mountain car\\nand robot reaching problems, which are arguably the hard-\\nest of the problems examined in this study. The ANGC’s\\neffectiveness on these problems is, we argue, largely due\\nto its own internally generated epistemic modulation factor\\nrep\\nt(this is empirically corroborated by the Instr-ANGC’s\\nworse performance than the full ANGC). In other words, the\\nANGC agent explores states that surprise it most, meaning it\\nis most “curious” about states that yield the highest magni-\\ntude total discrepancy (or the greatest free energy). This fea-\\nture presents a clean NGC implementation of the epistemic\\nterm key to the active inference framework (Friston et al.\\n2017b) which, theoretically, is meant to encourage a more\\nprincipled, efﬁcient form of environmental exploration. Fur-\\nthermore, this term, much akin to intrinsic curiosity models\\n(Pathak et al. 2017), would allow the agent to operate in set-\\ntings where even no external reward is available.\\nNote that the intent of this study was not to engage\\n34', metadata={'source': 'research.pdf', 'page': 5}),\n",
       " Document(page_content='(a) Inverted pendulum.\\n (b) Mountain car.\\n (c) Lunar lander.\\nFigure 3: Reward curves for ANGC and baselines (DQN, ICM, RnD, and the ablated model Instr-ANGC). Mean and standard\\ndeviation over 10 trials are plotted. Dash-dotted, horizontal (gray) lines depict the problem solution threshold.\\nFigure 4: The robot arm reaching problem results. Mean and standard deviation over 10trials plotted.\\nin a performance contest given that there is a vast array\\nof problem-tuned, neural-based RL approaches that attain\\nstate-of-the-art performance on many tasks. Instead, the in-\\ntent was to present a promising alternative to backprop-\\nbased approaches and demonstrate that ANGC can acquire\\ngood policies on control problems that DQN-based models\\ncan. Since the results are promising and our framework has\\nproven to be compatible with commonly-used RL heuris-\\ntics such as experience replay and target network stability,\\nintegrating other mechanisms would be a fruitful next step\\nto further improve performance. In the Appendix, we dis-\\ncuss the limitations of ANGC as well as its relationship with\\nfree energy optimization, examine ANGC in the context of\\nrelated work in RL and planning as inference, and conduct\\nfurther analysis on the control problems presented above.Conclusion\\nIn this paper, we proposed active neural generative cod-\\ning (ANGC), a framework for learning goal-directed agents\\nwithout backpropagation of errors (backprop). We demon-\\nstrated, on four control problems, the effectiveness of our\\napproach for learning systems that are competitive with\\nbackprop-based ones such as the deep Q-network and intrin-\\nsic curiosity variants. Notably, our framework demonstrates\\nthe value of leveraging the neuro-biologically grounded\\nlearning and inference mechanisms of neural generative\\ncoding to dynamically adapt a generative model that pro-\\nvides epistemic signals to augment problem-speciﬁc extrin-\\nsic rewards. Furthermore, given its robustness, ANGC could\\nprove useful in more complex environments, e.g., the Atari\\ngames, challenging robotics problems, offering an important\\nmeans of implementing longer-term planning-as-inference.\\n35500\\nEpisodic Reward\\n400\\n300\\n200\\n100\\n200\\n400\\n600\\n800\\n1000\\n# Episodes\\nANGC\\nDQN\\nICM\\nRandom\\nRnD\\nInstr-ANGC-100\\nEpisodic Reward\\n-120\\n-140\\n-160\\n-180\\n-200\\n200\\n400\\n600\\n800\\n1000\\n# Episodes\\nANGC\\nDQN\\nICM\\nRandom\\nRnD\\nInstr-ANGC300\\nReward\\n200\\n100\\nEpisodic\\n-100\\n-200\\n-300\\n-400\\n-500\\n200\\n400\\n600\\n800\\n1000\\n# Episodes\\nANGC\\nDQN\\nICM\\nRandom\\nRnD\\nInstr-ANGCEpisodic Reward\\n-10\\n200\\n400\\n600\\n800\\n1000\\n#Episodes\\nANGC\\nDQN\\nICM\\nRandom\\nRnD\\nInstr-ANGC', metadata={'source': 'research.pdf', 'page': 6}),\n",
       " Document(page_content='References\\nArulkumaran, K.; Deisenroth, M. P.; Brundage, M.; and\\nBharath, A. A. 2017. A brief survey of deep reinforcement\\nlearning. arXiv preprint arXiv:1708.05866.\\nBerlyne, D. E. 1966. Curiosity and exploration. Science,\\n153(3731): 25–33.\\nBotvinick, M.; and Toussaint, M. 2012. Planning as infer-\\nence. Trends in cognitive sciences, 16(10): 485–488.\\nBurda, Y .; Edwards, H.; Pathak, D.; Storkey, A.; Darrell,\\nT.; and Efros, A. A. 2018a. Large-scale study of curiosity-\\ndriven learning. arXiv preprint arXiv:1808.04355.\\nBurda, Y .; Edwards, H.; Storkey, A.; and Klimov, O. 2018b.\\nExploration by random network distillation. arXiv preprint\\narXiv:1810.12894.\\nClark, A. 2015. Surﬁng uncertainty: Prediction, action, and\\nthe embodied mind. Oxford University Press.\\nFriston, K. 2005. A theory of cortical responses. Philosophi-\\ncal transactions of the Royal Society B: Biological sciences ,\\n360(1456): 815–836.\\nFriston, K. 2009. The free-energy principle: a rough guide\\nto the brain? Trends in cognitive sciences, 13(7): 293–301.\\nFriston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.;\\nand Pezzulo, G. 2017a. Active inference: a process theory.\\nNeural computation, 29(1): 1–49.\\nFriston, K.; Mattout, J.; and Kilner, J. 2011. Action un-\\nderstanding and active inference. Biological cybernetics,\\n104(1): 137–160.\\nFriston, K.; Rigoli, F.; Ognibene, D.; Mathys, C.; Fitzgerald,\\nT.; and Pezzulo, G. 2015. Active inference and epistemic\\nvalue. Cognitive neuroscience, 6(4): 187–214.\\nFriston, K. J.; Daunizeau, J.; and Kiebel, S. J. 2009. Re-\\ninforcement learning or active inference? PloS one, 4(7):\\ne6421.\\nFriston, K. J.; Lin, M.; Frith, C. D.; Pezzulo, G.; Hobson,\\nJ. A.; and Ondobaka, S. 2017b. Active inference, curiosity\\nand insight. Neural computation, 29(10): 2633–2683.\\nFriston, K. J.; Rosch, R.; Parr, T.; Price, C.; and Bowman,\\nH. 2018. Deep temporal models and active inference. Neu-\\nroscience & Biobehavioral Reviews, 90: 486–501.\\nGuerguiev, J.; Lillicrap, T. P.; and Richards, B. A. 2017. To-\\nwards deep learning with segregated dendrites. ELife, 6:\\ne22901.\\nHa, D.; and Schmidhuber, J. 2018. Recurrent world\\nmodels facilitate policy evolution. arXiv preprint\\narXiv:1809.01999.\\nHafner, D.; Lillicrap, T.; Ba, J.; and Norouzi, M. 2019.\\nDream to control: Learning behaviors by latent imagination.\\narXiv preprint arXiv:1912.01603.\\nHeidrich-Meisner, V .; and Igel, C. 2009. Neuroevolution\\nstrategies for episodic reinforcement learning. Journal of\\nAlgorithms, 64(4): 152–168. Special Issue: Reinforcement\\nLearning.\\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980.Lake, B. M.; Ullman, T. D.; Tenenbaum, J. B.; and Gersh-\\nman, S. J. 2017. Building machines that learn and think like\\npeople. Behavioral and brain sciences, 40.\\nLee, D.-H.; Zhang, S.; Fischer, A.; and Bengio, Y . 2015. Dif-\\nference target propagation. In Joint European Conference on\\nMachine Learning and Knowledge Discovery in Databases ,\\n498–515. Springer.\\nLillicrap, T. P.; Cownden, D.; Tweed, D. B.; and Akerman,\\nC. J. 2016. Random synaptic feedback weights support error\\nbackpropagation for deep learning. Nature communications,\\n7: 13276.\\nManchev, N.; and Spratling, M. W. 2020. Target Propagation\\nin Recurrent Neural Networks. Journal of Machine Learn-\\ning Research, 21(7): 1–33.\\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;\\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-\\ning atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602.\\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-\\nland, A. K.; Ostrovski, G.; et al. 2015. Human-level control\\nthrough deep reinforcement learning. nature, 518(7540):\\n529–533.\\nMovellan, J. R. 1991. Contrastive Hebbian learning in the\\ncontinuous Hopﬁeld model. In Connectionist Models, 10–\\n17. Elsevier.\\nNajarro, E.; and Risi, S. 2020. Meta-learning through\\nhebbian plasticity in random networks. arXiv preprint\\narXiv:2007.02686.', metadata={'source': 'research.pdf', 'page': 7}),\n",
       " Document(page_content='continuous Hopﬁeld model. In Connectionist Models, 10–\\n17. Elsevier.\\nNajarro, E.; and Risi, S. 2020. Meta-learning through\\nhebbian plasticity in random networks. arXiv preprint\\narXiv:2007.02686.\\nNiv, Y . 2009. Reinforcement learning in the brain. Journal\\nof Mathematical Psychology, 53(3): 139–154.\\nO’Reilly, R. C. 1996. Biologically plausible error-driven\\nlearning using local activation differences: The generalized\\nrecirculation algorithm. Neural computation, 8(5): 895–938.\\nOrorbia, A.; and Kifer, D. 2020. The Neural Coding\\nFramework for Learning Generative Models. arXiv preprint\\narXiv:2012.03405.\\nOrorbia, A.; Mali, A.; Giles, C. L.; and Kifer, D. 2020a.\\nContinual learning of recurrent neural networks by locally\\naligning distributed representations. IEEE Transactions on\\nNeural Networks and Learning Systems.\\nOrorbia, A.; Mali, A.; Kifer, D.; and Giles, C. L. 2020b.\\nLarge-Scale Gradient-Free Deep Learning with Recursive\\nLocal Representation Alignment. arXiv e-prints, arXiv–\\n2002.\\nOrorbia, A. G.; Haffner, P.; Reitter, D.; and Giles, C. L.\\n2017. Learning to Adapt by Minimizing Discrepancy. arXiv\\npreprint arXiv:1711.11542.\\nOrorbia, A. G.; and Mali, A. 2019. Biologically motivated\\nalgorithms for propagating local target representations. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, volume 33, 4651–4658.\\nOudeyer, P.-Y . 2018. Computational theories of curiosity-\\ndriven learning. arXiv preprint arXiv:1802.10546.\\n36', metadata={'source': 'research.pdf', 'page': 7}),\n",
       " Document(page_content='O’Neill, J.; Pleydell-Bouverie, B.; Dupret, D.; and Csicsvari,\\nJ. 2010. Play it again: reactivation of waking experience and\\nmemory. Trends in neurosciences, 33(5): 220–229.\\nPathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017.\\nCuriosity-driven exploration by self-supervised prediction.\\nInInternational Conference on Machine Learning, 2778–\\n2787. PMLR.\\nRao, R. P.; and Ballard, D. H. 1999. Predictive coding in\\nthe visual cortex: a functional interpretation of some extra-\\nclassical receptive-ﬁeld effects. Nature neuroscience, 2(1).\\nRumelhart, D. E.; Hinton, G. E.; and Williams, R. J. 1986.\\nLearning representations by back-propagating errors. Na-\\nture, 323(6088): 533–536.\\nScellier, B.; and Bengio, Y . 2017. Equilibrium propagation:\\nBridging the gap between energy-based models and back-\\npropagation. Frontiers in computational neuroscience, 11:\\n24.\\nSekar, R.; Rybkin, O.; Daniilidis, K.; Abbeel, P.; Hafner, D.;\\nand Pathak, D. 2020. Planning to explore via self-supervised\\nworld models. In International Conference on Machine\\nLearning, 8583–8592. PMLR.\\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\\nT.; et al. 2018. A general reinforcement learning algorithm\\nthat masters chess, shogi, and Go through self-play. Science,\\n362(6419): 1140–1144.\\nSolway, A.; and Botvinick, M. M. 2012. Goal-directed de-\\ncision making as probabilistic inference: a computational\\nframework and potential neural correlates. Psychological\\nreview, 119(1): 120.\\nSpielberger, C. D.; and Starr, L. M. 1994. Curiosity and ex-\\nploratory behavior. Motivation: Theory and research, 221–\\n243.\\nSuch, F. P.; Madhavan, V .; Conti, E.; Lehman, J.; Stan-\\nley, K. O.; and Clune, J. 2017. Deep Neuroevolution: Ge-\\nnetic Algorithms Are a Competitive Alternative for Training\\nDeep Neural Networks for Reinforcement Learning. CoRR,\\nabs/1712.06567.\\nSutton, R. S. 1990. Integrated architectures for learning,\\nplanning, and reacting based on approximating dynamic\\nprogramming. In Machine learning proceedings 1990, 216–\\n224. Elsevier.\\nTieleman, T.; and Hinton, G. 2012. Lecture 6.5—RmsProp:\\nDivide the gradient by a running average of its recent magni-\\ntude. COURSERA: Neural Networks for Machine Learning.\\nTschantz, A.; Baltieri, M.; Seth, A. K.; and Buckley, C. L.\\n2020a. Scaling active inference. In 2020 International Joint\\nConference on Neural Networks (IJCNN), 1–8. IEEE.\\nTschantz, A.; Millidge, B.; Seth, A. K.; and Buckley, C. L.\\n2020b. Reinforcement learning through active inference.\\narXiv preprint arXiv:2002.12636.\\nTschantz, A.; Seth, A. K.; and Buckley, C. L. 2020. Learn-\\ning action-oriented models through active inference. PLoS\\ncomputational biology, 16(4): e1007805.\\nUeltzh ¨offer, K. 2018. Deep active inference. Biological cy-\\nbernetics, 112(6): 547–573.Whittington, J. C.; and Bogacz, R. 2017. An approximation\\nof the error backpropagation algorithm in a predictive cod-\\ning network with local hebbian synaptic plasticity. Neural\\ncomputation, 29(5): 1229–1262.\\nWiseman, S.; Chopra, S.; Ranzato, M.; Szlam, A.; Sun,\\nR.; Chintala, S.; and Vasilache, N. 2017. Training lan-\\nguage models using target-propagation. arXiv preprint\\narXiv:1702.04770.\\nWu, Y .; and Tian, Y . 2016. Training agent for ﬁrst-person\\nshooter game with actor-critic curriculum learning. N/A.\\nZador, A. M. 2019. A critique of pure learning and what ar-\\ntiﬁcial neural networks can learn from animal brains. Nature\\ncommunications, 10(1): 1–7.\\n37', metadata={'source': 'research.pdf', 'page': 8})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore  \n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "index_name = \"pdf-chatter\"\n",
    "embeddings = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "# To add documents to the Vector DB.\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name,namespace=pdf)\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Just to Load from the DB\n",
    "# index_name = \"pdf-chatter\"\n",
    "# pdf = \"ex.pdf\"\n",
    "# pdf_description = \"This is book of PHARMACOLOGY-II  B.Pharm, Semester-V.\"\n",
    "# docsearch = PineconeVectorStore(index_name=index_name, embedding=embeddings, namespace=pdf)\n",
    "# retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Memory Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000,return_messages=True)\n",
    "previous_history = \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a router to pick between tools. \n",
    " \n",
    "Cohere model decides which tool(s) to call, as well as the how to query them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Data model\n",
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "# Preamble\n",
    "preamble = f'''You are a master at finding information! You have two knowledge sources:\n",
    "        1. A massive document storage system containing detailed summaries ({pdf_description}) (vectorstore).\n",
    "        2. web_search.\n",
    "    When a user asks a question, if it directly relates to a document description in the storage system, use vectorstore to answer their question. \n",
    "    Otherwise, web_search for the best answer.'''\n",
    "# LLM with tool use and preamble\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_router = llm.bind_tools(tools=[web_search, vectorstore], preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "# response = question_router.invoke({\"question\": \"What is the capital of France?\"})\n",
    "# print(response.response_metadata['tool_calls'])\n",
    "# response = question_router.invoke({\"question\": \"What is the contact number of Tirth Patel?\"})\n",
    "# print(response.response_metadata['tool_calls'])\n",
    "# response = question_router.invoke({\"question\": \"Hi how are you?\"})\n",
    "# print('tool_calls' in response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datasource': 'web_search'}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Grader\n",
    "Grades every document which is relevant or not based on the question for the input of LLM to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# Prompt\n",
    "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, preamble=preamble)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question} \\n\\n Is the document relevant to the user question?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "# question = \"What is Deep Learning?\"\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[1].page_content\n",
    "# response =  retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generater LLM\n",
    "This simply generates answer based on the question given to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \",\n",
    "            additional_kwargs={\"documents\": x[\"documents\"]},\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# question = \"What is my name?\"\n",
    "# print(question)\n",
    "# print(docs)\n",
    "# history = [\n",
    "#     \"My name is Tirth Patel. I am a student at the University of Texas at Dallas. I am studying Computer Science.\"\n",
    "# ]\n",
    "# # Run\n",
    "# generation = rag_chain.invoke({\"documents\": docs, \"question\": question, \"history\": history})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM fallback\n",
    "When question is just a basic conversation than this is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM fallback\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"what is my name\"\n",
    "# history = [\n",
    "#     \" \"\n",
    "# ]\n",
    "# generation = llm_chain.invoke({\"question\": question, \"history\": history})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination Grader\n",
    "This's basic use is to tell wether the generated answer is a hallucination of the LLM or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Grader\n",
    "This is the Final check of the answer wether the generated answer serves the asked question of not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "# answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "\n",
    "Capture the flow in as a graph.\n",
    "\n",
    "## Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"|\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        history: list of chat history\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---LLM Fallback---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    generation = llm_chain.invoke({\"question\": question, \"history\": [previous_history]})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    if not isinstance(documents, list):\n",
    "      documents = [documents]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question, \"history\": [previous_history]})\n",
    "    print(generation)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    # Fallback to LLM or raise error if no decision\n",
    "    if \"tool_calls\" not in source.additional_kwargs:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"llm_fallback\"\n",
    "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "      raise \"Router could not decide source\"\n",
    "\n",
    "    # Choose datasource\n",
    "    datasource = source.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "    if datasource == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    else: \n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    print(\"dlsknflsdn\",type(score),score)\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY USING WEBSEARCH---\")\n",
    "        return \"not useful\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_chat(state):\n",
    "    \"\"\"\n",
    "    Save the chat history\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "    global previous_history\n",
    "    print(\"---SAVE CHAT---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    memory.save_context({\"input\": question}, {\"output\": generation})\n",
    "    # print(\"previous_history\",previous_history)\n",
    "    previous_history = memory.predict_new_summary(memory.chat_memory.messages, previous_history)\n",
    "    # print(\"previous_history\",previous_history)\n",
    "    memory.clear()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # rag\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback) # llm\n",
    "workflow.add_node(\"save_chat\", save_chat) # save chat\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"save_chat\", END)\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not useful\": \"web_search\", # Fails to answer question: fall-back to web-search \n",
    "        \"useful\": \"save_chat\", # Successfully answers question: save chat\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", \"save_chat\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO LLM---\n",
      "---LLM Fallback---\n",
      "---SAVE CHAT---\n",
      "previous_history The human asks about the AI's state of being. The AI responds that, while it doesn't have emotions or feelings, it is functioning as intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How Are you?',\n",
       " 'generation': 'I am an AI assistant, and I do not have emotions or feelings, but I am functioning as intended.'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"question\": \"How Are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO LLM---\n",
      "---LLM Fallback---\n",
      "---SAVE CHAT---\n",
      "previous_history The human asks about the AI's state of being and name. The AI responds that it is functioning as intended and without emotions or feelings. It also acknowledges the human's name as Tirth and states that it will remember this for future reference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'my name is tirth remenber it',\n",
       " 'generation': 'Ok, Tirth. I will remember your name.'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"question\": \"my name is tirth remenber it\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "The PDF is an academic paper about backprop-free reinforcement learning with active neural generative coding.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "---SAVE CHAT---\n",
      "previous_history The human, Tirth, inquires about the AI's state of being and name, to which the AI confirms it is functioning as intended, without emotions. It pledges to remember Tirth's name for future interactions. Tirth then asks about the content of a PDF, and the AI summarizes it as an academic paper on backprop-free reinforcement learning with active neural generative coding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is this given PDF about?',\n",
       " 'generation': 'The PDF is an academic paper about backprop-free reinforcement learning with active neural generative coding.',\n",
       " 'documents': [Document(page_content='References\\nArulkumaran, K.; Deisenroth, M. P.; Brundage, M.; and\\nBharath, A. A. 2017. A brief survey of deep reinforcement\\nlearning. arXiv preprint arXiv:1708.05866.\\nBerlyne, D. E. 1966. Curiosity and exploration. Science,\\n153(3731): 25–33.\\nBotvinick, M.; and Toussaint, M. 2012. Planning as infer-\\nence. Trends in cognitive sciences, 16(10): 485–488.\\nBurda, Y .; Edwards, H.; Pathak, D.; Storkey, A.; Darrell,\\nT.; and Efros, A. A. 2018a. Large-scale study of curiosity-\\ndriven learning. arXiv preprint arXiv:1808.04355.\\nBurda, Y .; Edwards, H.; Storkey, A.; and Klimov, O. 2018b.\\nExploration by random network distillation. arXiv preprint\\narXiv:1810.12894.\\nClark, A. 2015. Surﬁng uncertainty: Prediction, action, and\\nthe embodied mind. Oxford University Press.\\nFriston, K. 2005. A theory of cortical responses. Philosophi-\\ncal transactions of the Royal Society B: Biological sciences ,\\n360(1456): 815–836.\\nFriston, K. 2009. The free-energy principle: a rough guide\\nto the brain? Trends in cognitive sciences, 13(7): 293–301.\\nFriston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.;\\nand Pezzulo, G. 2017a. Active inference: a process theory.\\nNeural computation, 29(1): 1–49.\\nFriston, K.; Mattout, J.; and Kilner, J. 2011. Action un-\\nderstanding and active inference. Biological cybernetics,\\n104(1): 137–160.\\nFriston, K.; Rigoli, F.; Ognibene, D.; Mathys, C.; Fitzgerald,\\nT.; and Pezzulo, G. 2015. Active inference and epistemic\\nvalue. Cognitive neuroscience, 6(4): 187–214.\\nFriston, K. J.; Daunizeau, J.; and Kiebel, S. J. 2009. Re-\\ninforcement learning or active inference? PloS one, 4(7):\\ne6421.\\nFriston, K. J.; Lin, M.; Frith, C. D.; Pezzulo, G.; Hobson,\\nJ. A.; and Ondobaka, S. 2017b. Active inference, curiosity\\nand insight. Neural computation, 29(10): 2633–2683.\\nFriston, K. J.; Rosch, R.; Parr, T.; Price, C.; and Bowman,\\nH. 2018. Deep temporal models and active inference. Neu-\\nroscience & Biobehavioral Reviews, 90: 486–501.\\nGuerguiev, J.; Lillicrap, T. P.; and Richards, B. A. 2017. To-\\nwards deep learning with segregated dendrites. ELife, 6:\\ne22901.\\nHa, D.; and Schmidhuber, J. 2018. Recurrent world\\nmodels facilitate policy evolution. arXiv preprint\\narXiv:1809.01999.\\nHafner, D.; Lillicrap, T.; Ba, J.; and Norouzi, M. 2019.\\nDream to control: Learning behaviors by latent imagination.\\narXiv preprint arXiv:1912.01603.\\nHeidrich-Meisner, V .; and Igel, C. 2009. Neuroevolution\\nstrategies for episodic reinforcement learning. Journal of\\nAlgorithms, 64(4): 152–168. Special Issue: Reinforcement\\nLearning.\\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980.Lake, B. M.; Ullman, T. D.; Tenenbaum, J. B.; and Gersh-\\nman, S. J. 2017. Building machines that learn and think like\\npeople. Behavioral and brain sciences, 40.\\nLee, D.-H.; Zhang, S.; Fischer, A.; and Bengio, Y . 2015. Dif-\\nference target propagation. In Joint European Conference on\\nMachine Learning and Knowledge Discovery in Databases ,\\n498–515. Springer.\\nLillicrap, T. P.; Cownden, D.; Tweed, D. B.; and Akerman,\\nC. J. 2016. Random synaptic feedback weights support error\\nbackpropagation for deep learning. Nature communications,\\n7: 13276.\\nManchev, N.; and Spratling, M. W. 2020. Target Propagation\\nin Recurrent Neural Networks. Journal of Machine Learn-\\ning Research, 21(7): 1–33.\\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;\\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-\\ning atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602.\\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-\\nland, A. K.; Ostrovski, G.; et al. 2015. Human-level control\\nthrough deep reinforcement learning. nature, 518(7540):\\n529–533.\\nMovellan, J. R. 1991. Contrastive Hebbian learning in the\\ncontinuous Hopﬁeld model. In Connectionist Models, 10–\\n17. Elsevier.\\nNajarro, E.; and Risi, S. 2020. Meta-learning through\\nhebbian plasticity in random networks. arXiv preprint\\narXiv:2007.02686.', metadata={'page': 7.0, 'source': 'research.pdf'}),\n",
       "  Document(page_content='continuous Hopﬁeld model. In Connectionist Models, 10–\\n17. Elsevier.\\nNajarro, E.; and Risi, S. 2020. Meta-learning through\\nhebbian plasticity in random networks. arXiv preprint\\narXiv:2007.02686.\\nNiv, Y . 2009. Reinforcement learning in the brain. Journal\\nof Mathematical Psychology, 53(3): 139–154.\\nO’Reilly, R. C. 1996. Biologically plausible error-driven\\nlearning using local activation differences: The generalized\\nrecirculation algorithm. Neural computation, 8(5): 895–938.\\nOrorbia, A.; and Kifer, D. 2020. The Neural Coding\\nFramework for Learning Generative Models. arXiv preprint\\narXiv:2012.03405.\\nOrorbia, A.; Mali, A.; Giles, C. L.; and Kifer, D. 2020a.\\nContinual learning of recurrent neural networks by locally\\naligning distributed representations. IEEE Transactions on\\nNeural Networks and Learning Systems.\\nOrorbia, A.; Mali, A.; Kifer, D.; and Giles, C. L. 2020b.\\nLarge-Scale Gradient-Free Deep Learning with Recursive\\nLocal Representation Alignment. arXiv e-prints, arXiv–\\n2002.\\nOrorbia, A. G.; Haffner, P.; Reitter, D.; and Giles, C. L.\\n2017. Learning to Adapt by Minimizing Discrepancy. arXiv\\npreprint arXiv:1711.11542.\\nOrorbia, A. G.; and Mali, A. 2019. Biologically motivated\\nalgorithms for propagating local target representations. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, volume 33, 4651–4658.\\nOudeyer, P.-Y . 2018. Computational theories of curiosity-\\ndriven learning. arXiv preprint arXiv:1802.10546.\\n36', metadata={'page': 7.0, 'source': 'research.pdf'}),\n",
       "  Document(page_content='O’Neill, J.; Pleydell-Bouverie, B.; Dupret, D.; and Csicsvari,\\nJ. 2010. Play it again: reactivation of waking experience and\\nmemory. Trends in neurosciences, 33(5): 220–229.\\nPathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017.\\nCuriosity-driven exploration by self-supervised prediction.\\nInInternational Conference on Machine Learning, 2778–\\n2787. PMLR.\\nRao, R. P.; and Ballard, D. H. 1999. Predictive coding in\\nthe visual cortex: a functional interpretation of some extra-\\nclassical receptive-ﬁeld effects. Nature neuroscience, 2(1).\\nRumelhart, D. E.; Hinton, G. E.; and Williams, R. J. 1986.\\nLearning representations by back-propagating errors. Na-\\nture, 323(6088): 533–536.\\nScellier, B.; and Bengio, Y . 2017. Equilibrium propagation:\\nBridging the gap between energy-based models and back-\\npropagation. Frontiers in computational neuroscience, 11:\\n24.\\nSekar, R.; Rybkin, O.; Daniilidis, K.; Abbeel, P.; Hafner, D.;\\nand Pathak, D. 2020. Planning to explore via self-supervised\\nworld models. In International Conference on Machine\\nLearning, 8583–8592. PMLR.\\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\\nT.; et al. 2018. A general reinforcement learning algorithm\\nthat masters chess, shogi, and Go through self-play. Science,\\n362(6419): 1140–1144.\\nSolway, A.; and Botvinick, M. M. 2012. Goal-directed de-\\ncision making as probabilistic inference: a computational\\nframework and potential neural correlates. Psychological\\nreview, 119(1): 120.\\nSpielberger, C. D.; and Starr, L. M. 1994. Curiosity and ex-\\nploratory behavior. Motivation: Theory and research, 221–\\n243.\\nSuch, F. P.; Madhavan, V .; Conti, E.; Lehman, J.; Stan-\\nley, K. O.; and Clune, J. 2017. Deep Neuroevolution: Ge-\\nnetic Algorithms Are a Competitive Alternative for Training\\nDeep Neural Networks for Reinforcement Learning. CoRR,\\nabs/1712.06567.\\nSutton, R. S. 1990. Integrated architectures for learning,\\nplanning, and reacting based on approximating dynamic\\nprogramming. In Machine learning proceedings 1990, 216–\\n224. Elsevier.\\nTieleman, T.; and Hinton, G. 2012. Lecture 6.5—RmsProp:\\nDivide the gradient by a running average of its recent magni-\\ntude. COURSERA: Neural Networks for Machine Learning.\\nTschantz, A.; Baltieri, M.; Seth, A. K.; and Buckley, C. L.\\n2020a. Scaling active inference. In 2020 International Joint\\nConference on Neural Networks (IJCNN), 1–8. IEEE.\\nTschantz, A.; Millidge, B.; Seth, A. K.; and Buckley, C. L.\\n2020b. Reinforcement learning through active inference.\\narXiv preprint arXiv:2002.12636.\\nTschantz, A.; Seth, A. K.; and Buckley, C. L. 2020. Learn-\\ning action-oriented models through active inference. PLoS\\ncomputational biology, 16(4): e1007805.\\nUeltzh ¨offer, K. 2018. Deep active inference. Biological cy-\\nbernetics, 112(6): 547–573.Whittington, J. C.; and Bogacz, R. 2017. An approximation\\nof the error backpropagation algorithm in a predictive cod-\\ning network with local hebbian synaptic plasticity. Neural\\ncomputation, 29(5): 1229–1262.\\nWiseman, S.; Chopra, S.; Ranzato, M.; Szlam, A.; Sun,\\nR.; Chintala, S.; and Vasilache, N. 2017. Training lan-\\nguage models using target-propagation. arXiv preprint\\narXiv:1702.04770.\\nWu, Y .; and Tian, Y . 2016. Training agent for ﬁrst-person\\nshooter game with actor-critic curriculum learning. N/A.\\nZador, A. M. 2019. A critique of pure learning and what ar-\\ntiﬁcial neural networks can learn from animal brains. Nature\\ncommunications, 10(1): 1–7.\\n37', metadata={'page': 8.0, 'source': 'research.pdf'}),\n",
       "  Document(page_content='Backprop-Free Reinforcement Learning with Active Neural Generative Coding\\nAlexander G. Ororbia1, Ankur Mali2\\n1Rochester Institute of Technology\\n2The Pennsylvania State University\\nago@cs.rit.edu, aam35@psu.edu\\nAbstract\\nIn humans, perceptual awareness facilitates the fast recogni-\\ntion and extraction of information from sensory input. This\\nawareness largely depends on how the human agent interacts\\nwith the environment. In this work, we propose active neural\\ngenerative coding, a computational framework for learning\\naction-driven generative models without backpropagation of\\nerrors (backprop) in dynamic environments. Speciﬁcally, we\\ndevelop an intelligent agent that operates even with sparse re-\\nwards, drawing inspiration from the cognitive theory of plan-\\nning as inference. We demonstrate on several simple control\\nproblems that our framework performs competitively with\\ndeep Q-learning. The robust performance of our agent offers\\npromising evidence that a backprop-free approach for neural\\ninference and learning can drive goal-directed behavior.\\nIntroduction\\nManipulating one’s environment in the effort to understand\\nit is an essential ingredient of learning in humans (Spiel-\\nberger and Starr 1994; Berlyne 1966). In cognitive neuro-\\nscience, behavioral and neurobiological evidence indicates a\\ndistinction between goal-directed and habitual action selec-\\ntion in reward-based decision-making. With respect to ha-\\nbitual action selection, or actions taken based on situation-\\nresponse associations, ample evidence exists to support the\\ntemporal-difference (TD) account from reinforcement learn-\\ning. In this account, the neurotransmitter dopamine creates\\nan error signal based on reward prediction to drive (state) up-\\ndates in the corpus striatum, a particular neuronal region in\\nthe basal ganglia that affects an agent’s choice of action. In\\ncontrast, goal-directed action requires prospective planning\\nwhere actions are taken based on predictions of their future\\npotential outcomes (Niv 2009; Solway and Botvinick 2012).\\nPlanning-as-inference (PAI) (Botvinick and Toussaint 2012)\\nattempts to account for goal-directed behavior by casting it\\nas a problem of probabilistic inference where an agent ma-\\nnipulates an internal model that estimates the probability of\\npotential action-outcome-reward sequences.\\nOne important, emerging theoretical framework for PAI is\\nthat of active inference (Friston, Mattout, and Kilner 2011;\\nTschantz, Seth, and Buckley 2020), which posits that bio-\\nlogical agents learn a probabilistic generative model by in-\\nCopyright © 2022, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.teracting with their world, adjusting the internal states of this\\nmodel to account for the evidence that they acquire from\\ntheir environment. This scheme uniﬁes perception, action,\\nand learning in adaptive systems by framing them as pro-\\ncesses that result from approximate Bayesian inference, ele-\\ngantly tackling the exploration-exploitation trade-off inher-\\nent to organism survival. The emergence of this framework\\nis timely – in reinforcement learning (RL) research, despite\\nthe recent successes afforded by artiﬁcial neural networks\\n(ANNs) (Mnih et al. 2013; Silver et al. 2018), most models\\nrequire exorbitant quantities of data to train well, struggling\\nto learn tasks as efﬁciently as humans and animals (Arulku-\\nmaran et al. 2017). As a result, a key challenge is how to\\ndesign RL methods that successfully resolve environmental\\nuncertainty and complexity given limited resources and data.\\nModel-based RL, explored in statistical learning research\\nthrough world (Ha and Schmidhuber 2018) or dynamics\\nmodels (Sutton 1990), offers a promising means of tack-\\nling this challenge and active inference provides a promising\\npath towards instantiating it in a powerful yet neurocogni-\\ntively meaningful way (Tschantz et al. 2020b).\\nAlthough PAI and active inference offer an excellent story', metadata={'page': 0.0, 'source': 'research.pdf'})]}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"question\": \"What is this given PDF about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n"
     ]
    },
    {
     "ename": "TooManyRequestsError",
     "evalue": "status_code: 429, body: {'message': \"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is Mechanism of Action? also specify the page number.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Node\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNode \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:686\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug)\u001b[0m\n\u001b[1;32m    679\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    680\u001b[0m     futures,\n\u001b[1;32m    681\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    682\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    683\u001b[0m )\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    689\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1049\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3961\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3969\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3971\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3835\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3833\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3835\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[177], line 75\u001b[0m, in \u001b[0;36mgrade_documents\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 75\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_grader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     grade \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mbinary_score\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grade \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:154\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    151\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    153\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    164\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:556\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    550\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    554\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    555\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:417\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    416\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    418\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    419\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    421\u001b[0m ]\n\u001b[1;32m    422\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 407\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:626\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 626\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/langchain_cohere/chat_models.py:365\u001b[0m, in \u001b[0;36mChatCohere._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    362\u001b[0m request \u001b[38;5;241m=\u001b[39m get_cohere_chat_request(\n\u001b[1;32m    363\u001b[0m     messages, stop_sequences\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 365\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_info(response)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_info:\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/cohere/client.py:27\u001b[0m, in \u001b[0;36mvalidate_args.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m     26\u001b[0m     check_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/PDF-chat/.venv/lib/python3.11/site-packages/cohere/base_client.py:655\u001b[0m, in \u001b[0;36mBaseCohere.chat\u001b[0;34m(self, message, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, temperature, max_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, tools, tool_results, request_options)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pydantic\u001b[38;5;241m.\u001b[39mparse_obj_as(NonStreamedChatResponse, _response\u001b[38;5;241m.\u001b[39mjson())  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(pydantic\u001b[38;5;241m.\u001b[39mparse_obj_as(typing\u001b[38;5;241m.\u001b[39mAny, _response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     _response_json \u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: status_code: 429, body: {'message': \"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"what is Mechanism of Action? also specify the page number.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/623da7bb-84a7-4e53-a63e-7ccd77fb9be5/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "The authors of this book are Dr. K.V. Otari, Dr. Pragnesh Patani, and Dr. Pankaj Mishra.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The authors of this book are Dr. K.V. Otari, Dr. Pragnesh Patani, and Dr. '\n",
      " 'Pankaj Mishra.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/57f3973b-6879-4fbe-ae31-9ae524c3a697/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "તીર્થ પટેલ.\n",
      "---CHECK HALLUCINATIONS---\n",
      "dlsknflsdn <class '__main__.GradeHallucinations'> binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'તીર્થ પટેલ.'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"Translate Tirth Patel in Gujarati.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having a Memory component Is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = VectorStoreRetrieverMemory()\n",
    "conversation_chain = ConversationChain(workflow, memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
